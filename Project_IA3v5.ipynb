{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxoS3ks6jQ18"
      },
      "source": [
        "# Setup libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rHY9MlLZm9_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df13b462-b713-4e62-c855-e6fe2feed1b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy tensorflow tqdm matplotlib torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-K4-3hj74t_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZy-QwGqjVBM"
      },
      "source": [
        "# Setup neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNA2SCwUjZBs"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    \"\"\"Simple CNN for CIFAR-10 classification\"\"\"\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # 32x32 -> 16x16\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 16x16 -> 8x8\n",
        "        x = self.pool(F.relu(self.conv3(x)))  # 8x8 -> 4x4\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oEJO58nlP33"
      },
      "source": [
        "# Attack implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rngWwdfoldfg"
      },
      "source": [
        "PGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXirJG1alflw"
      },
      "outputs": [],
      "source": [
        "def custom_pgd_attack(model, images, labels, epsilon=8/255, alpha=2/255, num_iter=10):\n",
        "    \"\"\"Custom PGD implementation\"\"\"\n",
        "    images = images.clone().detach().to(images.device)\n",
        "    labels = labels.clone().detach().to(labels.device)\n",
        "\n",
        "    delta = torch.empty_like(images).uniform_(-epsilon, epsilon)\n",
        "    delta.requires_grad = True\n",
        "\n",
        "    for _ in range(num_iter):\n",
        "        outputs = model(images + delta)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        grad_sign = delta.grad.data.sign()\n",
        "        delta.data = delta.data + alpha * grad_sign\n",
        "        delta.data = torch.clamp(delta.data, -epsilon, epsilon)\n",
        "        delta.data = torch.clamp(images.data + delta.data, 0, 1) - images.data\n",
        "\n",
        "        delta.grad.zero_()\n",
        "\n",
        "    return images + delta.detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rlrv3t3DlgA0"
      },
      "source": [
        "DeepFool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGYjeGOcllfF"
      },
      "outputs": [],
      "source": [
        "def custom_deepfool_attack(model, image, num_classes=10, overshoot=0.02, max_iter=50):\n",
        "    \"\"\"Custom DeepFool implementation\"\"\"\n",
        "    image = image.clone().detach().unsqueeze(0)\n",
        "    image.requires_grad = True\n",
        "\n",
        "    model.eval()\n",
        "    original_output = model(image)\n",
        "    original_label = original_output.argmax(dim=1).item()\n",
        "\n",
        "    perturb = torch.zeros_like(image)\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        outputs = model(image + perturb)\n",
        "        current_label = outputs.argmax(dim=1).item()\n",
        "\n",
        "        if current_label != original_label:\n",
        "            break\n",
        "\n",
        "        grads = []\n",
        "        for k in range(num_classes):\n",
        "            if k == original_label:\n",
        "                continue\n",
        "\n",
        "            if image.grad is not None:\n",
        "                image.grad.zero_()\n",
        "\n",
        "            outputs[0, k].backward(retain_graph=True)\n",
        "            grad = image.grad.clone()\n",
        "            grads.append(grad)\n",
        "\n",
        "        min_distance = float('inf')\n",
        "        min_grad = None\n",
        "        original_score = outputs[0, original_label]\n",
        "\n",
        "        for i, k in enumerate([j for j in range(num_classes) if j != original_label]):\n",
        "            w = grads[i].flatten()\n",
        "            f = outputs[0, k] - original_score\n",
        "            distance = abs(f.item()) / (torch.norm(w).item() + 1e-10)\n",
        "\n",
        "            if distance < min_distance:\n",
        "                min_distance = distance\n",
        "                min_grad = grads[i]\n",
        "\n",
        "        if min_grad is not None:\n",
        "            r = (min_distance + 1e-4) * min_grad / (torch.norm(min_grad) + 1e-10)\n",
        "            perturb = perturb + r\n",
        "\n",
        "    return (image + (1 + overshoot) * perturb).squeeze(0).detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defense implementations"
      ],
      "metadata": {
        "id": "NVDP-aor6gHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_jacobian_regularization(model, images, labels, lambda_reg=0.01):\n",
        "    \"\"\"\n",
        "    Compute Jacobian regularization term\n",
        "    Encourages smoothness in model predictions\n",
        "\n",
        "    Args:\n",
        "        model: Neural network model\n",
        "        images: Input images\n",
        "        labels: True labels\n",
        "        lambda_reg: Regularization strength\n",
        "\n",
        "    Returns:\n",
        "        Total loss (cross-entropy + Jacobian regularization)\n",
        "    \"\"\"\n",
        "    images.requires_grad = True\n",
        "    outputs = model(images)\n",
        "\n",
        "    # Standard cross-entropy loss\n",
        "    ce_loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "    # Compute Jacobian regularization (Frobenius norm of gradients)\n",
        "    jacobian_reg = 0\n",
        "    num_classes = outputs.size(1)\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        if images.grad is not None:\n",
        "            images.grad.zero_()\n",
        "\n",
        "        outputs_i = outputs[:, i].sum()\n",
        "        outputs_i.backward(retain_graph=True)\n",
        "\n",
        "        grad_norm = torch.norm(images.grad, p=2)\n",
        "        jacobian_reg += grad_norm ** 2\n",
        "\n",
        "    jacobian_reg = jacobian_reg / (num_classes * images.size(0))\n",
        "\n",
        "    total_loss = ce_loss + lambda_reg * jacobian_reg\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "eo-i4LKw6fcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training procedures"
      ],
      "metadata": {
        "id": "20_UJP236rk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normal training"
      ],
      "metadata": {
        "id": "l59Qh4SR6vql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the k of the folds\n",
        "def train_standard(model, dataset, optimizer, device, k=5, epochs=10, patience=3):\n",
        "    \"\"\"Training with K-Fold Cross-Validation and Early Stopping\"\"\"\n",
        "    kfold = KFold(n_splits=k, shuffle=True)\n",
        "\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
        "        print(f'\\nFold {fold + 1}/{k}')\n",
        "\n",
        "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
        "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
        "        train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=64, shuffle=False)\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        epochs_no_improve = 0\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs} [Train]')\n",
        "            for images, labels in pbar:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = F.cross_entropy(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                pbar.set_postfix({'loss': total_loss/total, 'acc': 100.*correct/total})\n",
        "\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pbar_val = tqdm(val_loader, desc=f'Fold {fold + 1} [Val]')\n",
        "                for images, labels in pbar_val:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                    outputs = model(images)\n",
        "                    loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                    pbar_val.set_postfix({'val_loss': val_loss/val_total, 'val_acc': 100.*val_correct/val_total})\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                epochs_no_improve = 0\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                break\n",
        "\n",
        "        model.load_state_dict(best_model_wts)\n",
        "\n",
        "        fold_results.append({\n",
        "            'train_loss': total_loss / len(train_loader),\n",
        "            'train_acc': 100. * correct / total,\n",
        "            'val_loss': avg_val_loss,\n",
        "            'val_acc': 100. * val_correct / val_total\n",
        "        })\n",
        "\n",
        "    avg_train_loss = sum(f['train_loss'] for f in fold_results) / k\n",
        "    avg_train_acc = sum(f['train_acc'] for f in fold_results) / k\n",
        "    avg_val_loss = sum(f['val_loss'] for f in fold_results) / k\n",
        "    avg_val_acc = sum(f['val_acc'] for f in fold_results) / k\n",
        "\n",
        "    print(f\"\\nAverage Results after {k} folds:\")\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_acc:.2f}%\")\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_acc:.2f}%\")\n",
        "\n",
        "    return avg_train_loss, avg_train_acc"
      ],
      "metadata": {
        "id": "jOT5nCXnn2Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adversarial training"
      ],
      "metadata": {
        "id": "945tpA-j6ywB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_adversarial(model, train_loader, optimizer, device, epoch, epsilon=8/255):\n",
        "    \"\"\"Adversarial training defense - train on adversarial examples\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch} [Adversarial Training]')\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Generate adversarial examples\n",
        "        model.eval()  # Set to eval mode for attack generation\n",
        "        adv_images = custom_pgd_attack(model, images, labels, epsilon=epsilon)\n",
        "        model.train()  # Back to train mode\n",
        "\n",
        "        # Train on adversarial examples\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(adv_images)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        pbar.set_postfix({'loss': total_loss/total, 'acc': 100.*correct/total})\n",
        "\n",
        "    return total_loss / len(train_loader), 100. * correct / total"
      ],
      "metadata": {
        "id": "YAUaJIon63pX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def train_adversarial(model, dataset, optimizer, device, attack_type, epochs=10, k=5, patience=3, epsilon=8/255):\n",
        "    \"\"\"Adversarial training defense - Train on adversarial examples with K-Fold Cross-Validation and Early Stopping\"\"\"\n",
        "\n",
        "    # Hardcoded parameters for KFold and early stopping\n",
        "    kfold = KFold(n_splits=k, shuffle=True)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
        "        print(f'\\nFold {fold + 1}/{k}')\n",
        "\n",
        "        # Prepare the train and validation data loaders for this fold\n",
        "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
        "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
        "        train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=64, shuffle=False)\n",
        "\n",
        "        # Initialize early stopping parameters\n",
        "        best_val_loss = float('inf')\n",
        "        epochs_no_improve = 0\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        # Training loop for each fold\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs} [Adversarial Training]')\n",
        "            for images, labels in pbar:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                # Generate adversarial examples using attack\n",
        "                model.eval()  # Set to eval mode for attack generation\n",
        "                adv_images = None\n",
        "                if attack_type == 'pgd':\n",
        "                  adv_images = custom_pgd_attack(model, images, labels, epsilon=epsilon)\n",
        "                elif attack_type == 'deepfool':\n",
        "                  adv_images_list = []\n",
        "                  for i in range(images.size(0)):\n",
        "                    adv_images_list.append(custom_deepfool_attack(model, images[i]))\n",
        "                  adv_images = torch.stack(adv_images_list)\n",
        "                elif attack_type == 'none':\n",
        "                  adv_images = images\n",
        "                elif attack_type == 'both':\n",
        "                  adv_images_pgd = custom_pgd_attack(model, images, labels, epsilon=epsilon)\n",
        "                  adv_images_deepfool_list = []\n",
        "                  for i in range(images.size(0)):\n",
        "                    adv_images_deepfool_list.append(custom_deepfool_attack(model, images[i]))\n",
        "                  adv_images_deepfool = torch.stack(adv_images_deepfool_list)\n",
        "\n",
        "                  # Combine the adversarial images for random sampling\n",
        "                  # Convert tensors to lists of individual images, combine, then stack back\n",
        "                  combined_adv_images_list = adv_images_pgd.unbind(0) + adv_images_deepfool.unbind(0)\n",
        "                  selected_adv_images_list = random.sample(combined_adv_images_list, len(images))\n",
        "                  adv_images = torch.stack(selected_adv_images_list)\n",
        "                model.train()  # Back to train mode\n",
        "\n",
        "                # Train on adversarial examples\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(adv_images)\n",
        "                loss = F.cross_entropy(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                pbar.set_postfix({'loss': total_loss / total, 'acc': 100. * correct / total})\n",
        "\n",
        "            # Validation step after each epoch\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pbar_val = tqdm(val_loader, desc=f'Fold {fold + 1} [Validation]')\n",
        "                for images, labels in pbar_val:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                    # Perform validation on original images (not adversarial)\n",
        "                    outputs = model(images)\n",
        "                    loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                    pbar_val.set_postfix({'val_loss': val_loss / val_total, 'val_acc': 100. * val_correct / val_total})\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "            # Early stopping check\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                epochs_no_improve = 0\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())  # Save the best model weights\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            # If no improvement for 'patience' epochs, stop early\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                break\n",
        "\n",
        "        # Restore the best model weights after training\n",
        "        model.load_state_dict(best_model_wts)\n",
        "\n",
        "        # Store the results for this fold\n",
        "        fold_results.append({\n",
        "            'train_loss': total_loss / len(train_loader),\n",
        "            'train_acc': 100. * correct / total,\n",
        "        })\n",
        "\n",
        "    # Compute average results across all folds\n",
        "    avg_train_loss = sum(f['train_loss'] for f in fold_results) / k\n",
        "    avg_train_acc = sum(f['train_acc'] for f in fold_results) / k\n",
        "\n",
        "    print(f\"\\nAverage Results after {k} folds:\")\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_acc:.2f}%\")\n",
        "\n",
        "    return avg_train_loss, avg_train_acc"
      ],
      "metadata": {
        "id": "CV014hF6opAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jacobian reg. training"
      ],
      "metadata": {
        "id": "s_IFwpYH649f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_jacobian(model, train_loader, optimizer, device, epoch, lambda_reg=0.01):\n",
        "    \"\"\"Train with Jacobian regularization defense\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch} [Jacobian Reg]')\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = compute_jacobian_regularization(model, images, labels, lambda_reg)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        with torch.no_grad():\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': total_loss/total, 'acc': 100.*correct/total})\n",
        "\n",
        "    return total_loss / len(train_loader), 100. * correct / total\n",
        "\n"
      ],
      "metadata": {
        "id": "4_cyQAf6675P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DA K OF DA FOLDS\n",
        "def train_jacobian(model, dataset, optimizer, device, epochs=10, k=5, patience=3, lambda_reg=0.01):\n",
        "    \"\"\"Train with Jacobian regularization defense with K-Fold Cross-Validation and Early Stopping\"\"\"\n",
        "\n",
        "    # Hardcoded parameters for KFold and early stopping\n",
        "    kfold = KFold(n_splits=k, shuffle=True)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
        "        print(f'\\nFold {fold + 1}/{k}')\n",
        "\n",
        "        # Prepare the train and validation data loaders for this fold\n",
        "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
        "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
        "        train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=64, shuffle=False)\n",
        "\n",
        "        # Initialize early stopping parameters\n",
        "        best_val_loss = float('inf')\n",
        "        epochs_no_improve = 0\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        # Training loop for each fold\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs} [Jacobian Regularization]')\n",
        "            for images, labels in pbar:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss = compute_jacobian_regularization(model, images, labels, lambda_reg)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = model(images).max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                pbar.set_postfix({'loss': total_loss / total, 'acc': 100. * correct / total})\n",
        "\n",
        "            # Validation step after each epoch\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pbar_val = tqdm(val_loader, desc=f'Fold {fold + 1} [Validation]')\n",
        "                for images, labels in pbar_val:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                    # Perform validation on original images (not adversarial)\n",
        "                    outputs = model(images)\n",
        "                    loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                    pbar_val.set_postfix({'val_loss': val_loss / val_total, 'val_acc': 100. * val_correct / val_total})\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "            # Early stopping check\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                epochs_no_improve = 0\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())  # Save the best model weights\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            # If no improvement for 'patience' epochs, stop early\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                break\n",
        "\n",
        "        # Restore the best model weights after training\n",
        "        model.load_state_dict(best_model_wts)\n",
        "\n",
        "        # Store the results for this fold\n",
        "        fold_results.append({\n",
        "            'train_loss': total_loss / len(train_loader),\n",
        "            'train_acc': 100. * correct / total,\n",
        "        })\n",
        "\n",
        "    # Compute average results across all folds\n",
        "    avg_train_loss = sum(f['train_loss'] for f in fold_results) / k\n",
        "    avg_train_acc = sum(f['train_acc'] for f in fold_results) / k\n",
        "\n",
        "    print(f\"\\nAverage Results after {k} folds:\")\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_acc:.2f}%\")\n",
        "\n",
        "    return avg_train_loss, avg_train_acc"
      ],
      "metadata": {
        "id": "6J4pYgJRo3Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "EN8AHKkW7Iy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def deepfool_batch(model, images):\n",
        "    # Helper function to apply custom_deepfool_attack to a batch of images\n",
        "    adv_images_list = []\n",
        "    for i in range(images.size(0)):\n",
        "        adv_images_list.append(custom_deepfool_attack(model, images[i]))\n",
        "    return torch.stack(adv_images_list)\n",
        "\n",
        "def evaluate(model, test_loader, device, attack_type=None, epsilon=8/255):\n",
        "    \"\"\"\n",
        "    Evaluate model on clean or adversarial examples\n",
        "\n",
        "    Args:\n",
        "        model: Neural network model\n",
        "        test_loader: Test data loader\n",
        "        device: Device to run on\n",
        "        attack_type: 'pgd', 'deepfool', or None for clean evaluation\n",
        "        epsilon: Attack strength\n",
        "\n",
        "    Returns:\n",
        "        Accuracy percentage, time taken\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    desc = f'Evaluating [{attack_type if attack_type else \"Clean\"}]'\n",
        "    pbar = tqdm(test_loader, desc=desc)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Generate adversarial examples if attack specified\n",
        "        if attack_type == 'pgd':\n",
        "            images = images.clone().detach().to(images.device)\n",
        "            labels = labels.clone().detach().to(labels.device)\n",
        "            images = custom_pgd_attack(model, images, labels, epsilon=epsilon)\n",
        "        elif attack_type == 'deepfool':\n",
        "             with torch.no_grad(): # DeepFool does not require gradients for the original images in the evaluation loop\n",
        "                 images = deepfool_batch(model, images)\n",
        "\n",
        "        with torch.no_grad(): # Evaluate the model without gradient computation\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            pbar.set_postfix({'acc': 100.*correct/total})\n",
        "\n",
        "    end_time = time.time()\n",
        "    accuracy = 100. * correct / total\n",
        "    time_taken = end_time - start_time\n",
        "    return accuracy, time_taken"
      ],
      "metadata": {
        "id": "fwJ-saVJ7NCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main experiment using implementations"
      ],
      "metadata": {
        "id": "njQxcOYg7OU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Load data\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train\n",
        ")\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "# Create model\n",
        "model = SimpleCNN().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxKwAZNH96IO",
        "outputId": "40277bcc-28d5-4b02-806b-de6e252471e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count = 10 # Define the number of epochs for the k-fold training functions\n",
        "\n",
        "train_loss, train_acc = train_standard(model, train_dataset, optimizer, device, k=5, epochs=epoch_count, patience=3)\n",
        "print(f'Standard Training Complete: Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%')\n",
        "\n",
        "# Evaluate\n",
        "clean_acc = evaluate(model, test_loader, device, attack_type=None)\n",
        "pgd_acc = evaluate(model, test_loader, device, attack_type='pgd', epsilon=8/255)\n",
        "\n",
        "print(f'Clean Accuracy: {clean_acc[0]:.2f}%')\n",
        "print(f'PGD Attack Accuracy: {pgd_acc[0]:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8s0-jkR7S99",
        "outputId": "a84614da-fe67-499b-ea70-444dfb7031b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.25it/s, loss=0.0107, acc=76.1]\n",
            "Fold 1 [Val]: 100%|██████████| 157/157 [00:03<00:00, 49.36it/s, val_loss=0.00997, val_acc=78]\n",
            "Epoch 2/10 [Train]: 100%|██████████| 625/625 [00:15<00:00, 39.21it/s, loss=0.0108, acc=76.4]\n",
            "Fold 1 [Val]: 100%|██████████| 157/157 [00:03<00:00, 50.26it/s, val_loss=0.00991, val_acc=77.7]\n",
            "Epoch 3/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.12it/s, loss=0.0106, acc=76.7]\n",
            "Fold 1 [Val]: 100%|██████████| 157/157 [00:03<00:00, 43.87it/s, val_loss=0.00936, val_acc=79.2]\n",
            "Epoch 4/10 [Train]: 100%|██████████| 625/625 [00:15<00:00, 41.24it/s, loss=0.0105, acc=77]\n",
            "Fold 1 [Val]: 100%|██████████| 157/157 [00:03<00:00, 51.40it/s, val_loss=0.0105, val_acc=76.5]\n",
            "Epoch 5/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 41.99it/s, loss=0.0105, acc=76.8]\n",
            "Fold 1 [Val]: 100%|██████████| 157/157 [00:03<00:00, 43.79it/s, val_loss=0.00978, val_acc=78.4]\n",
            "Epoch 6/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 41.71it/s, loss=0.0102, acc=77.4]\n",
            "Fold 1 [Val]: 100%|██████████| 157/157 [00:03<00:00, 50.85it/s, val_loss=0.00949, val_acc=79.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 6\n",
            "\n",
            "Fold 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.08it/s, loss=0.0107, acc=76.4]\n",
            "Fold 2 [Val]: 100%|██████████| 157/157 [00:03<00:00, 42.97it/s, val_loss=0.0095, val_acc=78.8]\n",
            "Epoch 2/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 41.96it/s, loss=0.0105, acc=76.8]\n",
            "Fold 2 [Val]: 100%|██████████| 157/157 [00:03<00:00, 50.51it/s, val_loss=0.00965, val_acc=78.6]\n",
            "Epoch 3/10 [Train]: 100%|██████████| 625/625 [00:15<00:00, 41.62it/s, loss=0.0103, acc=77.3]\n",
            "Fold 2 [Val]: 100%|██████████| 157/157 [00:03<00:00, 42.99it/s, val_loss=0.0102, val_acc=78]\n",
            "Epoch 4/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 41.84it/s, loss=0.0102, acc=77.5]\n",
            "Fold 2 [Val]: 100%|██████████| 157/157 [00:03<00:00, 51.75it/s, val_loss=0.00927, val_acc=79.5]\n",
            "Epoch 5/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.29it/s, loss=0.0101, acc=77.7]\n",
            "Fold 2 [Val]: 100%|██████████| 157/157 [00:03<00:00, 42.20it/s, val_loss=0.00934, val_acc=79.5]\n",
            "Epoch 6/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.19it/s, loss=0.0101, acc=77.8]\n",
            "Fold 2 [Val]: 100%|██████████| 157/157 [00:03<00:00, 50.84it/s, val_loss=0.00975, val_acc=78.8]\n",
            "Epoch 7/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 41.85it/s, loss=0.00996, acc=78.3]\n",
            "Fold 2 [Val]: 100%|██████████| 157/157 [00:03<00:00, 42.69it/s, val_loss=0.0096, val_acc=78.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 7\n",
            "\n",
            "Fold 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.31it/s, loss=0.0103, acc=77.2]\n",
            "Fold 3 [Val]: 100%|██████████| 157/157 [00:03<00:00, 51.88it/s, val_loss=0.00907, val_acc=79.8]\n",
            "Epoch 2/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.25it/s, loss=0.0101, acc=77.8]\n",
            "Fold 3 [Val]: 100%|██████████| 157/157 [00:03<00:00, 42.72it/s, val_loss=0.00923, val_acc=79.9]\n",
            "Epoch 3/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.34it/s, loss=0.01, acc=78.1]\n",
            "Fold 3 [Val]: 100%|██████████| 157/157 [00:03<00:00, 51.74it/s, val_loss=0.00879, val_acc=80.4]\n",
            "Epoch 4/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.13it/s, loss=0.00996, acc=77.9]\n",
            "Fold 3 [Val]: 100%|██████████| 157/157 [00:03<00:00, 42.94it/s, val_loss=0.00891, val_acc=79.9]\n",
            "Epoch 5/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.40it/s, loss=0.00998, acc=78.1]\n",
            "Fold 3 [Val]: 100%|██████████| 157/157 [00:03<00:00, 51.97it/s, val_loss=0.0089, val_acc=80]\n",
            "Epoch 6/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.53it/s, loss=0.00974, acc=78.6]\n",
            "Fold 3 [Val]: 100%|██████████| 157/157 [00:03<00:00, 43.13it/s, val_loss=0.00895, val_acc=79.9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 6\n",
            "\n",
            "Fold 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.41it/s, loss=0.01, acc=77.8]\n",
            "Fold 4 [Val]: 100%|██████████| 157/157 [00:03<00:00, 49.82it/s, val_loss=0.00913, val_acc=79.7]\n",
            "Epoch 2/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.42it/s, loss=0.00985, acc=78.1]\n",
            "Fold 4 [Val]: 100%|██████████| 157/157 [00:03<00:00, 42.80it/s, val_loss=0.00866, val_acc=80.6]\n",
            "Epoch 3/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.53it/s, loss=0.00983, acc=78.2]\n",
            "Fold 4 [Val]: 100%|██████████| 157/157 [00:03<00:00, 51.60it/s, val_loss=0.00853, val_acc=80.9]\n",
            "Epoch 4/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.35it/s, loss=0.00981, acc=78.6]\n",
            "Fold 4 [Val]: 100%|██████████| 157/157 [00:03<00:00, 43.07it/s, val_loss=0.0089, val_acc=80.5]\n",
            "Epoch 5/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.49it/s, loss=0.00962, acc=78.9]\n",
            "Fold 4 [Val]: 100%|██████████| 157/157 [00:03<00:00, 51.23it/s, val_loss=0.00902, val_acc=79.9]\n",
            "Epoch 6/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.46it/s, loss=0.00961, acc=78.6]\n",
            "Fold 4 [Val]: 100%|██████████| 157/157 [00:03<00:00, 44.10it/s, val_loss=0.00898, val_acc=80]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 6\n",
            "\n",
            "Fold 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 41.92it/s, loss=0.00982, acc=78.2]\n",
            "Fold 5 [Val]: 100%|██████████| 157/157 [00:03<00:00, 50.39it/s, val_loss=0.00801, val_acc=82.3]\n",
            "Epoch 2/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.08it/s, loss=0.00975, acc=78.5]\n",
            "Fold 5 [Val]: 100%|██████████| 157/157 [00:03<00:00, 44.15it/s, val_loss=0.00829, val_acc=82.3]\n",
            "Epoch 3/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 41.81it/s, loss=0.00968, acc=78.5]\n",
            "Fold 5 [Val]: 100%|██████████| 157/157 [00:03<00:00, 51.05it/s, val_loss=0.0086, val_acc=81.2]\n",
            "Epoch 4/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.35it/s, loss=0.00959, acc=78.9]\n",
            "Fold 5 [Val]: 100%|██████████| 157/157 [00:03<00:00, 43.26it/s, val_loss=0.00887, val_acc=80.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 4\n",
            "\n",
            "Average Results after 5 folds:\n",
            "Train Loss: 0.6289, Train Accuracy: 78.36%\n",
            "Validation Loss: 0.5847, Validation Accuracy: 79.62%\n",
            "Standard Training Complete: Loss=0.6289, Train Acc=78.36%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating [Clean]: 100%|██████████| 79/79 [00:01<00:00, 50.65it/s, acc=80.4]\n",
            "Evaluating [pgd]: 100%|██████████| 79/79 [00:04<00:00, 19.09it/s, acc=0.03]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean Accuracy: 80.36%\n",
            "PGD Attack Accuracy: 0.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new model\n",
        "model_adv = SimpleCNN().to(device)\n",
        "optimizer_adv = torch.optim.Adam(model_adv.parameters(), lr=0.001)\n",
        "\n",
        "# Train with adversarial examples\n",
        "epoch_count = 10 # Number of epochs for the k-fold training\n",
        "train_loss, train_acc = train_adversarial(model_adv, train_dataset, optimizer_adv, device, attack_type='pgd', k=5, epochs=epoch_count, patience=3, epsilon=8/255)\n",
        "print(f'Adversarial Training Complete: Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%')\n",
        "\n",
        "# Evaluate\n",
        "clean_acc = evaluate(model_adv, test_loader, device, attack_type=None)\n",
        "pgd_acc = evaluate(model_adv, test_loader, device, attack_type='pgd', epsilon=8/255)\n",
        "\n",
        "print(f'Clean Accuracy: {clean_acc[0]:.2f}%')\n",
        "print(f'PGD Attack Accuracy: {pgd_acc[0]:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-plGkIrhTtMd",
        "outputId": "dce0d3f7-f6b1-4c28-e6fa-bbcc09ca7c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 20.88it/s, loss=0.0344, acc=17.7]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 43.63it/s, val_loss=0.0308, val_acc=29.2]\n",
            "Epoch 2/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.47it/s, loss=0.033, acc=22.1]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 51.45it/s, val_loss=0.0289, val_acc=36]\n",
            "Epoch 3/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.72it/s, loss=0.0323, acc=23.9]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 50.71it/s, val_loss=0.0276, val_acc=37.5]\n",
            "Epoch 4/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.65it/s, loss=0.032, acc=24.3]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 50.57it/s, val_loss=0.0278, val_acc=37]\n",
            "Epoch 5/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.81it/s, loss=0.0318, acc=24.8]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 42.70it/s, val_loss=0.0276, val_acc=40.2]\n",
            "Epoch 6/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 20.85it/s, loss=0.0316, acc=25.3]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 51.18it/s, val_loss=0.0271, val_acc=39.5]\n",
            "Epoch 7/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.58it/s, loss=0.0315, acc=25.4]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 50.77it/s, val_loss=0.0267, val_acc=39.1]\n",
            "Epoch 8/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.58it/s, loss=0.0313, acc=25.8]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 49.92it/s, val_loss=0.0266, val_acc=42.2]\n",
            "Epoch 9/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.78it/s, loss=0.0312, acc=25.9]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 42.20it/s, val_loss=0.0266, val_acc=41.4]\n",
            "Epoch 10/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 20.85it/s, loss=0.0311, acc=26.1]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 47.89it/s, val_loss=0.0261, val_acc=41.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.75it/s, loss=0.0311, acc=26.4]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 51.05it/s, val_loss=0.0261, val_acc=42.5]\n",
            "Epoch 2/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.66it/s, loss=0.031, acc=26.4]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 50.79it/s, val_loss=0.026, val_acc=41.7]\n",
            "Epoch 3/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 20.95it/s, loss=0.0309, acc=26.5]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 43.44it/s, val_loss=0.0256, val_acc=42.7]\n",
            "Epoch 4/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 20.96it/s, loss=0.0308, acc=26.6]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 49.20it/s, val_loss=0.0259, val_acc=42.5]\n",
            "Epoch 5/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.71it/s, loss=0.0307, acc=27]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 51.09it/s, val_loss=0.0254, val_acc=43.9]\n",
            "Epoch 6/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.75it/s, loss=0.0308, acc=26.7]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 50.11it/s, val_loss=0.0262, val_acc=41.6]\n",
            "Epoch 7/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 20.97it/s, loss=0.0307, acc=26.8]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 42.73it/s, val_loss=0.0253, val_acc=43.3]\n",
            "Epoch 8/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 20.97it/s, loss=0.0307, acc=27.1]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 48.54it/s, val_loss=0.0256, val_acc=44.2]\n",
            "Epoch 9/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.74it/s, loss=0.0306, acc=27.1]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 51.15it/s, val_loss=0.0254, val_acc=44]\n",
            "Epoch 10/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.71it/s, loss=0.0306, acc=27.2]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 50.02it/s, val_loss=0.025, val_acc=45]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 20.90it/s, loss=0.0306, acc=27]\n",
            "Fold 3 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 44.16it/s, val_loss=0.0254, val_acc=44.2]\n",
            "Epoch 2/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 21.01it/s, loss=0.0306, acc=27.3]\n",
            "Fold 3 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 48.21it/s, val_loss=0.0248, val_acc=46.1]\n",
            "Epoch 3/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.83it/s, loss=0.0306, acc=27.3]\n",
            "Fold 3 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 51.57it/s, val_loss=0.0253, val_acc=46.6]\n",
            "Epoch 4/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.73it/s, loss=0.0305, acc=27.4]\n",
            "Fold 3 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 51.31it/s, val_loss=0.0249, val_acc=46.4]\n",
            "Epoch 5/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 21.05it/s, loss=0.0305, acc=27.5]\n",
            "Fold 3 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 42.15it/s, val_loss=0.025, val_acc=44.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 5\n",
            "\n",
            "Fold 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.73it/s, loss=0.0305, acc=27.4]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 47.02it/s, val_loss=0.0254, val_acc=45.2]\n",
            "Epoch 2/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.69it/s, loss=0.0305, acc=27.3]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 50.51it/s, val_loss=0.0253, val_acc=44.2]\n",
            "Epoch 3/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.65it/s, loss=0.0304, acc=27.7]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 51.12it/s, val_loss=0.0249, val_acc=44.7]\n",
            "Epoch 4/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 20.90it/s, loss=0.0305, acc=27.4]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 43.15it/s, val_loss=0.0255, val_acc=44.6]\n",
            "Epoch 5/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 20.89it/s, loss=0.0304, acc=27.4]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 47.46it/s, val_loss=0.0251, val_acc=45.9]\n",
            "Epoch 6/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.65it/s, loss=0.0304, acc=27.8]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 50.91it/s, val_loss=0.025, val_acc=45.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 6\n",
            "\n",
            "Fold 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.51it/s, loss=0.0305, acc=27.3]\n",
            "Fold 5 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 50.73it/s, val_loss=0.0248, val_acc=45.3]\n",
            "Epoch 2/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.72it/s, loss=0.0305, acc=27.5]\n",
            "Fold 5 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 45.00it/s, val_loss=0.0252, val_acc=45.3]\n",
            "Epoch 3/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 20.84it/s, loss=0.0305, acc=27.8]\n",
            "Fold 5 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 43.63it/s, val_loss=0.0254, val_acc=45.6]\n",
            "Epoch 4/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 20.93it/s, loss=0.0304, acc=27.6]\n",
            "Fold 5 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 51.13it/s, val_loss=0.025, val_acc=45.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 4\n",
            "\n",
            "Average Results after 5 folds:\n",
            "Train Loss: 1.9592, Train Accuracy: 27.23%\n",
            "Adversarial Training Complete: Loss=1.9592, Train Acc=27.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating [Clean]: 100%|██████████| 79/79 [00:01<00:00, 51.52it/s, acc=46.9]\n",
            "Evaluating [pgd]: 100%|██████████| 79/79 [00:04<00:00, 17.33it/s, acc=30]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean Accuracy: 46.88%\n",
            "PGD Attack Accuracy: 29.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new model\n",
        "model_jac = SimpleCNN().to(device)\n",
        "optimizer_jac = torch.optim.Adam(model_jac.parameters(), lr=0.001)\n",
        "\n",
        "# Train with Jacobian regularization\n",
        "epoch_count = 10 # Number of epochs for the k-fold training\n",
        "train_loss, train_acc = train_jacobian(model_jac, train_dataset, optimizer_jac, device, k=5, epochs=epoch_count, patience=3, lambda_reg=0.01)\n",
        "print(f'Jacobian Regularization Training Complete: Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%')\n",
        "\n",
        "# Evaluate\n",
        "clean_acc = evaluate(model_jac, test_loader, device, attack_type=None)\n",
        "pgd_acc = evaluate(model_jac, test_loader, device, attack_type='pgd', epsilon=8/255)\n",
        "\n",
        "print(f'Clean Accuracy: {clean_acc[0]:.2f}%')\n",
        "print(f'PGD Attack Accuracy: {pgd_acc[0]:.2f}%')"
      ],
      "metadata": {
        "id": "8j6qAH28TuVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "# Test PGD attack\n",
        "adv_images_pgd = custom_pgd_attack(model, images, labels, epsilon=8/255)\n",
        "\n",
        "# Test DeepFool attack (on single image)\n",
        "single_image = images[0]\n",
        "adv_image_deepfool = custom_deepfool_attack(model, single_image)\n",
        "\n",
        "# Compare predictions\n",
        "with torch.no_grad():\n",
        "    clean_pred = model(images).argmax(dim=1)\n",
        "    pgd_pred = model(adv_images_pgd).argmax(dim=1)\n",
        "\n",
        "print(\"Clean predictions:\", clean_pred[:10])\n",
        "print(\"PGD adversarial predictions:\", pgd_pred[:10])\n",
        "print(\"True labels:\", labels[:10])"
      ],
      "metadata": {
        "id": "HFJ5FzkmTyTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get one image\n",
        "img = images[0].cpu()\n",
        "adv_img = adv_images_pgd[0].cpu()\n",
        "\n",
        "# Calculate perturbation\n",
        "perturbation = (adv_img - img).abs()\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "axes[0].imshow(img.permute(1, 2, 0))\n",
        "axes[0].set_title('Clean Image')\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(adv_img.permute(1, 2, 0))\n",
        "axes[1].set_title('Adversarial Image')\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(perturbation.permute(1, 2, 0) * 10)  # Amplified for visibility\n",
        "axes[2].set_title('Perturbation (10x)')\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Max perturbation: {perturbation.max():.4f}\")\n",
        "print(f\"L2 norm: {perturbation.norm():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "1TbHcqU3T1Im",
        "outputId": "3324f106-e8d8-4706-a0ba-78f96b4788a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max perturbation: 0.0314\n",
            "L2 norm: 1.4193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkNFYK4vsTPK"
      },
      "source": [
        "# State of the art implementations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install adversarial-robustness-toolbox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu9aEbbuY4CJ",
        "outputId": "2e2432dc-5b8c-4ab6-aaf6-831d3fd138c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adversarial-robustness-toolbox\n",
            "  Downloading adversarial_robustness_toolbox-1.20.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (1.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (75.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (3.6.0)\n",
            "Downloading adversarial_robustness_toolbox-1.20.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: adversarial-robustness-toolbox\n",
            "Successfully installed adversarial-robustness-toolbox-1.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# ART imports\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "from art.attacks.evasion import ProjectedGradientDescent, DeepFool"
      ],
      "metadata": {
        "id": "-SEH0W2Sbiv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparisson to custom implementation"
      ],
      "metadata": {
        "id": "w3bo3ty-Y0O4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_custom_pgd(model, test_loader, device, epsilon=8/255, max_samples=1000):\n",
        "    \"\"\"Evaluate using custom PGD implementation\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    times = []\n",
        "\n",
        "    for images, labels in tqdm(test_loader, desc='Custom PGD'):\n",
        "        if total >= max_samples:\n",
        "            break\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        start_time = time.time()\n",
        "        adv_images = custom_pgd_attack(model, images, labels, epsilon=epsilon)\n",
        "        times.append(time.time() - start_time)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(adv_images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    avg_time = np.mean(times)\n",
        "    return accuracy, avg_time\n",
        "\n",
        "\n",
        "def evaluate_art_pgd(classifier, test_loader, epsilon=8/255, max_samples=1000):\n",
        "    \"\"\"Evaluate using ART PGD implementation\"\"\"\n",
        "    pgd_attack = ProjectedGradientDescent(\n",
        "        estimator=classifier,\n",
        "        eps=epsilon,\n",
        "        eps_step=epsilon/4,\n",
        "        max_iter=10,\n",
        "        num_random_init=1,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    times = []\n",
        "\n",
        "    for images, labels in tqdm(test_loader, desc='ART PGD'):\n",
        "        if total >= max_samples:\n",
        "            break\n",
        "\n",
        "        x_batch = images.numpy()\n",
        "        y_batch = labels.numpy()\n",
        "\n",
        "        start_time = time.time()\n",
        "        x_adv = pgd_attack.generate(x=x_batch)\n",
        "        times.append(time.time() - start_time)\n",
        "\n",
        "        predictions = classifier.predict(x_adv)\n",
        "        predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "        total += len(y_batch)\n",
        "        correct += np.sum(predicted_labels == y_batch)\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    avg_time = np.mean(times)\n",
        "    return accuracy, avg_time\n",
        "\n",
        "\n",
        "\n",
        "def calculate_perturbation_metrics(model, test_loader, device, attack_fn, epsilon=8/255, max_samples=100):\n",
        "    \"\"\"Calculate L2 and L-inf norms of perturbations\"\"\"\n",
        "    model.eval()\n",
        "    l2_norms = []\n",
        "    linf_norms = []\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        if len(l2_norms) >= max_samples:\n",
        "            break\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        if 'custom' in attack_fn.__name__:\n",
        "            adv_images = attack_fn(model, images, labels, epsilon=epsilon)\n",
        "        else:\n",
        "            # For DeepFool\n",
        "            for i in range(images.size(0)):\n",
        "                if len(l2_norms) >= max_samples:\n",
        "                    break\n",
        "                adv_img = attack_fn(model, images[i])\n",
        "                perturbation = (adv_img - images[i]).abs()\n",
        "                l2_norms.append(perturbation.norm(p=2).item())\n",
        "                linf_norms.append(perturbation.max().item())\n",
        "            continue\n",
        "\n",
        "        perturbation = (adv_images - images).abs()\n",
        "        for i in range(perturbation.size(0)):\n",
        "            if len(l2_norms) >= max_samples:\n",
        "                break\n",
        "            l2_norms.append(perturbation[i].norm(p=2).item())\n",
        "            linf_norms.append(perturbation[i].max().item())\n",
        "\n",
        "    return {\n",
        "        'l2_mean': np.mean(l2_norms),\n",
        "        'l2_std': np.std(l2_norms),\n",
        "        'linf_mean': np.mean(linf_norms),\n",
        "        'linf_std': np.std(linf_norms)\n",
        "    }"
      ],
      "metadata": {
        "id": "-yl6RqYQFZz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_custom_deepfool(model, test_loader, device, max_samples=200):\n",
        "    \"\"\"Evaluate using custom DeepFool implementation\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    times = []\n",
        "\n",
        "    for images, labels in tqdm(test_loader, desc='Custom DeepFool'):\n",
        "        if total >= max_samples:\n",
        "            break\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        for i in range(images.size(0)):\n",
        "            if total >= max_samples:\n",
        "                break\n",
        "\n",
        "            start_time = time.time()\n",
        "            adv_img = custom_deepfool_attack(model, images[i])\n",
        "            times.append(time.time() - start_time)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output = model(adv_img.unsqueeze(0))\n",
        "                predicted = output.argmax(dim=1).item()\n",
        "                total += 1\n",
        "                if predicted == labels[i].item():\n",
        "                    correct += 1\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    avg_time = np.mean(times)\n",
        "    return accuracy, avg_time\n",
        "\n",
        "\n",
        "def evaluate_art_deepfool(classifier, test_loader, max_samples=200):\n",
        "    \"\"\"Evaluate using ART DeepFool implementation\"\"\"\n",
        "    deepfool_attack = DeepFool(\n",
        "        classifier=classifier,\n",
        "        max_iter=50,\n",
        "        epsilon=1e-6,\n",
        "        nb_grads=10,\n",
        "        batch_size=1,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    times = []\n",
        "\n",
        "    for images, labels in tqdm(test_loader, desc='ART DeepFool'):\n",
        "        if total >= max_samples:\n",
        "            break\n",
        "\n",
        "        x_batch = images.numpy()\n",
        "        y_batch = labels.numpy()\n",
        "\n",
        "        start_time = time.time()\n",
        "        x_adv = deepfool_attack.generate(x=x_batch)\n",
        "        times.append(time.time() - start_time)\n",
        "\n",
        "        predictions = classifier.predict(x_adv)\n",
        "        predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "        total += len(y_batch)\n",
        "        correct += np.sum(predicted_labels == y_batch)\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    avg_time = np.mean(times)\n",
        "    return accuracy, avg_time"
      ],
      "metadata": {
        "id": "Idbg3pAsxwbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"=\"*70)\n",
        "    print(\"CUSTOM vs STATE-OF-THE-ART ADVERSARIAL ML COMPARISON\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'\\nUsing device: {device}')\n",
        "\n",
        "    # Load data\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=False, download=True, transform=transform\n",
        "    )\n",
        "    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Load or train a simple model\n",
        "    print(\"\\nTraining a simple model for comparison...\")\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=True, download=True, transform=transform\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "    model = SimpleCNN().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Quick training (3 epochs)\n",
        "    for epoch in range(1, 4):\n",
        "        model.train()\n",
        "        for images, labels in tqdm(train_loader, desc=f'Training Epoch {epoch}'):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluate clean accuracy\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    clean_acc = 100. * correct / total\n",
        "    print(f\"\\nClean Accuracy: {clean_acc:.2f}%\")\n",
        "\n",
        "    # Wrap model in ART classifier\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    classifier = PyTorchClassifier(\n",
        "        model=model,\n",
        "        clip_values=(0, 1),\n",
        "        loss=criterion,\n",
        "        optimizer=optimizer,\n",
        "        input_shape=(3, 32, 32),\n",
        "        nb_classes=10,\n",
        "    )\n",
        "\n",
        "    epsilon = 8/255\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PGD ATTACK COMPARISON\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Custom PGD\n",
        "    print(\"\\n[1/2] Evaluating Custom PGD...\")\n",
        "    custom_pgd_acc, custom_pgd_time = evaluate_custom_pgd(\n",
        "        model, test_loader, device, epsilon=epsilon, max_samples=1000\n",
        "    )\n",
        "\n",
        "    # ART PGD\n",
        "    print(\"\\n[2/2] Evaluating ART PGD...\")\n",
        "    art_pgd_acc, art_pgd_time = evaluate_art_pgd(\n",
        "        classifier, test_loader, epsilon=epsilon, max_samples=1000\n",
        "    )\n",
        "\n",
        "    # Calculate perturbation metrics for PGD\n",
        "    print(\"\\nCalculating perturbation metrics for PGD...\")\n",
        "    custom_pgd_metrics = calculate_perturbation_metrics(\n",
        "        model, test_loader, device, custom_pgd_attack, epsilon=epsilon, max_samples=100\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"PGD RESULTS:\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"{'Metric':<30} {'Custom':<20} {'ART (SOTA)':<20}\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"{'Accuracy':<30} {custom_pgd_acc:<20.2f} {art_pgd_acc:<20.2f}\")\n",
        "    print(f\"{'Avg Time per Batch (s)':<30} {custom_pgd_time:<20.4f} {art_pgd_time:<20.4f}\")\n",
        "    print(f\"{'L2 Norm (mean)':<30} {custom_pgd_metrics['l2_mean']:<20.4f} {'N/A':<20}\")\n",
        "    print(f\"{'L-inf Norm (mean)':<30} {custom_pgd_metrics['linf_mean']:<20.4f} {'N/A':<20}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    improvement_acc = art_pgd_acc - custom_pgd_acc\n",
        "    improvement_time = ((custom_pgd_time - art_pgd_time) / custom_pgd_time) * 100\n",
        "\n",
        "    print(f\"\\nART vs Custom:\")\n",
        "    print(f\"  Accuracy difference: {improvement_acc:+.2f}%\")\n",
        "    print(f\"  Speed improvement: {improvement_time:+.1f}%\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"DEEPFOOL ATTACK COMPARISON\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Custom DeepFool\n",
        "    print(\"\\n[1/2] Evaluating Custom DeepFool...\")\n",
        "    custom_df_acc, custom_df_time = evaluate_custom_deepfool(\n",
        "        model, test_loader, device, max_samples=200\n",
        "    )\n",
        "\n",
        "    # ART DeepFool\n",
        "    print(\"\\n[2/2] Evaluating ART DeepFool...\")\n",
        "    art_df_acc, art_df_time = evaluate_art_deepfool(\n",
        "        classifier, test_loader, max_samples=200\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"DEEPFOOL RESULTS:\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"{'Metric':<30} {'Custom':<20} {'ART (SOTA)':<20}\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"{'Accuracy':<30} {custom_df_acc:<20.2f} {art_df_acc:<20.2f}\")\n",
        "    print(f\"{'Avg Time per Sample (s)':<30} {custom_df_time:<20.4f} {art_df_time:<20.4f}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    improvement_acc_df = art_df_acc - custom_df_acc\n",
        "    improvement_time_df = ((custom_df_time - art_df_time) / custom_df_time) * 100\n",
        "\n",
        "    print(f\"\\nART vs Custom:\")\n",
        "    print(f\"  Accuracy difference: {improvement_acc_df:+.2f}%\")\n",
        "    print(f\"  Speed improvement: {improvement_time_df:+.1f}%\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nClean Accuracy: {clean_acc:.2f}%\")\n",
        "    print(f\"\\nPGD Attack:\")\n",
        "    print(f\"  Custom:    {custom_pgd_acc:.2f}% (defender wins if higher)\")\n",
        "    print(f\"  ART SOTA:  {art_pgd_acc:.2f}%\")\n",
        "    print(f\"\\nDeepFool Attack:\")\n",
        "    print(f\"  Custom:    {custom_df_acc:.2f}%\")\n",
        "    print(f\"  ART SOTA:  {art_df_acc:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "1pqGPDV2Fght",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a041d68-b8bf-4adc-e137-c773ee8d355c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CUSTOM vs STATE-OF-THE-ART ADVERSARIAL ML COMPARISON\n",
            "======================================================================\n",
            "\n",
            "Using device: cpu\n",
            "\n",
            "Training a simple model for comparison...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 391/391 [01:53<00:00,  3.44it/s]\n",
            "Training Epoch 2: 100%|██████████| 391/391 [01:53<00:00,  3.45it/s]\n",
            "Training Epoch 3: 100%|██████████| 391/391 [01:53<00:00,  3.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Clean Accuracy: 62.98%\n",
            "\n",
            "======================================================================\n",
            "PGD ATTACK COMPARISON\n",
            "======================================================================\n",
            "\n",
            "[1/2] Evaluating Custom PGD...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Custom PGD:  10%|█         | 10/100 [00:24<03:36,  2.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2/2] Evaluating ART PGD...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ART PGD:  10%|█         | 10/100 [00:30<04:31,  3.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating perturbation metrics for PGD...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "PGD RESULTS:\n",
            "----------------------------------------------------------------------\n",
            "Metric                         Custom               ART (SOTA)          \n",
            "----------------------------------------------------------------------\n",
            "Accuracy                       3.00                 14.80               \n",
            "Avg Time per Batch (s)         2.2823               2.8840              \n",
            "L2 Norm (mean)                 1.5352               N/A                 \n",
            "L-inf Norm (mean)              0.0314               N/A                 \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "ART vs Custom:\n",
            "  Accuracy difference: +11.80%\n",
            "  Speed improvement: -26.4%\n",
            "\n",
            "======================================================================\n",
            "DEEPFOOL ATTACK COMPARISON\n",
            "======================================================================\n",
            "\n",
            "[1/2] Evaluating Custom DeepFool...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Custom DeepFool:   2%|▏         | 2/100 [01:23<1:08:12, 41.76s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2/2] Evaluating ART DeepFool...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ART DeepFool:   2%|▏         | 2/100 [02:50<2:19:17, 85.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "DEEPFOOL RESULTS:\n",
            "----------------------------------------------------------------------\n",
            "Metric                         Custom               ART (SOTA)          \n",
            "----------------------------------------------------------------------\n",
            "Accuracy                       20.00                26.50               \n",
            "Avg Time per Sample (s)        0.4147               85.0185             \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "ART vs Custom:\n",
            "  Accuracy difference: +6.50%\n",
            "  Speed improvement: -20403.7%\n",
            "\n",
            "======================================================================\n",
            "SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Clean Accuracy: 62.98%\n",
            "\n",
            "PGD Attack:\n",
            "  Custom:    3.00% (defender wins if higher)\n",
            "  ART SOTA:  14.80%\n",
            "\n",
            "DeepFool Attack:\n",
            "  Custom:    20.00%\n",
            "  ART SOTA:  26.50%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}