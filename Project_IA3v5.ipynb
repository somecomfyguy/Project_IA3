{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxoS3ks6jQ18"
      },
      "source": [
        "# Setup libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "rHY9MlLZm9_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d30784-8619-4840-d947-1776ac2b7c77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy tensorflow tqdm matplotlib torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D-K4-3hj74t_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import KFold\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZy-QwGqjVBM"
      },
      "source": [
        "# Setup neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pNA2SCwUjZBs"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    \"\"\"Simple CNN for CIFAR-10 classification\"\"\"\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # 32x32 -> 16x16\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 16x16 -> 8x8\n",
        "        x = self.pool(F.relu(self.conv3(x)))  # 8x8 -> 4x4\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oEJO58nlP33"
      },
      "source": [
        "# Attack implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rngWwdfoldfg"
      },
      "source": [
        "PGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DXirJG1alflw"
      },
      "outputs": [],
      "source": [
        "def custom_pgd_attack(model, images, labels, epsilon=8/255, alpha=2/255, num_iter=10):\n",
        "    \"\"\"Custom PGD implementation\"\"\"\n",
        "    images = images.clone().detach().to(images.device)\n",
        "    labels = labels.clone().detach().to(labels.device)\n",
        "\n",
        "    delta = torch.empty_like(images).uniform_(-epsilon, epsilon)\n",
        "    delta.requires_grad = True\n",
        "\n",
        "    for _ in range(num_iter):\n",
        "        outputs = model(images + delta)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        grad_sign = delta.grad.data.sign()\n",
        "        delta.data = delta.data + alpha * grad_sign\n",
        "        delta.data = torch.clamp(delta.data, -epsilon, epsilon)\n",
        "        delta.data = torch.clamp(images.data + delta.data, 0, 1) - images.data\n",
        "\n",
        "        delta.grad.zero_()\n",
        "\n",
        "    return images + delta.detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rlrv3t3DlgA0"
      },
      "source": [
        "DeepFool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kGYjeGOcllfF"
      },
      "outputs": [],
      "source": [
        "def custom_deepfool_attack(model, image, num_classes=10, overshoot=0.02, max_iter=50):\n",
        "    \"\"\"Custom DeepFool implementation\"\"\"\n",
        "    image = image.clone().detach().unsqueeze(0)\n",
        "    image.requires_grad = True\n",
        "\n",
        "    model.eval()\n",
        "    original_output = model(image)\n",
        "    original_label = original_output.argmax(dim=1).item()\n",
        "\n",
        "    perturb = torch.zeros_like(image)\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        outputs = model(image + perturb)\n",
        "        current_label = outputs.argmax(dim=1).item()\n",
        "\n",
        "        if current_label != original_label:\n",
        "            break\n",
        "\n",
        "        grads = []\n",
        "        for k in range(num_classes):\n",
        "            if k == original_label:\n",
        "                continue\n",
        "\n",
        "            if image.grad is not None:\n",
        "                image.grad.zero_()\n",
        "\n",
        "            outputs[0, k].backward(retain_graph=True)\n",
        "            grad = image.grad.clone()\n",
        "            grads.append(grad)\n",
        "\n",
        "        min_distance = float('inf')\n",
        "        min_grad = None\n",
        "        original_score = outputs[0, original_label]\n",
        "\n",
        "        for i, k in enumerate([j for j in range(num_classes) if j != original_label]):\n",
        "            w = grads[i].flatten()\n",
        "            f = outputs[0, k] - original_score\n",
        "            distance = abs(f.item()) / (torch.norm(w).item() + 1e-10)\n",
        "\n",
        "            if distance < min_distance:\n",
        "                min_distance = distance\n",
        "                min_grad = grads[i]\n",
        "\n",
        "        if min_grad is not None:\n",
        "            r = (min_distance + 1e-4) * min_grad / (torch.norm(min_grad) + 1e-10)\n",
        "            perturb = perturb + r\n",
        "\n",
        "    return (image + (1 + overshoot) * perturb).squeeze(0).detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defense implementations"
      ],
      "metadata": {
        "id": "NVDP-aor6gHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_jacobian_regularization(model, images, labels, lambda_reg=0.01):\n",
        "    \"\"\"\n",
        "    Compute Jacobian regularization term\n",
        "    Encourages smoothness in model predictions\n",
        "    \"\"\"\n",
        "    # Create a copy that requires gradients (don't modify original)\n",
        "    images_copy = images.detach().clone()\n",
        "    images_copy.requires_grad = True\n",
        "\n",
        "    outputs = model(images_copy)\n",
        "\n",
        "    # Standard cross-entropy loss\n",
        "    ce_loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "    # Compute Jacobian regularization (Frobenius norm of gradients)\n",
        "    jacobian_reg = 0\n",
        "    num_classes = outputs.size(1)\n",
        "\n",
        "    for i in range(num_classes):\n",
        "        # Zero out gradients before each computation\n",
        "        if images_copy.grad is not None:\n",
        "            images_copy.grad.zero_()\n",
        "\n",
        "        # Compute gradient for this class\n",
        "        outputs_i = outputs[:, i].sum()\n",
        "        outputs_i.backward(retain_graph=True)\n",
        "\n",
        "        if images_copy.grad is not None:\n",
        "            grad_norm = torch.norm(images_copy.grad, p=2)\n",
        "            jacobian_reg += grad_norm ** 2\n",
        "\n",
        "    jacobian_reg = jacobian_reg / (num_classes * images.size(0))\n",
        "\n",
        "    total_loss = ce_loss + lambda_reg * jacobian_reg\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "eo-i4LKw6fcm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training procedures"
      ],
      "metadata": {
        "id": "20_UJP236rk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normal training"
      ],
      "metadata": {
        "id": "l59Qh4SR6vql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the k of the folds\n",
        "def train_standard(model, dataset, optimizer, device, k=5, epochs=10, patience=3):\n",
        "    \"\"\"Training with K-Fold Cross-Validation and Early Stopping\"\"\"\n",
        "    kfold = KFold(n_splits=k, shuffle=True)\n",
        "\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
        "        print(f'\\nFold {fold + 1}/{k}')\n",
        "\n",
        "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
        "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
        "        train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=64, shuffle=False)\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        epochs_no_improve = 0\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs} [Train]')\n",
        "            for images, labels in pbar:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(images)\n",
        "                loss = F.cross_entropy(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                pbar.set_postfix({'loss': total_loss/total, 'acc': 100.*correct/total})\n",
        "\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pbar_val = tqdm(val_loader, desc=f'Fold {fold + 1} [Val]')\n",
        "                for images, labels in pbar_val:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                    outputs = model(images)\n",
        "                    loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                    pbar_val.set_postfix({'val_loss': val_loss/val_total, 'val_acc': 100.*val_correct/val_total})\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                epochs_no_improve = 0\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                break\n",
        "\n",
        "        model.load_state_dict(best_model_wts)\n",
        "\n",
        "        fold_results.append({\n",
        "            'train_loss': total_loss / len(train_loader),\n",
        "            'train_acc': 100. * correct / total,\n",
        "            'val_loss': avg_val_loss,\n",
        "            'val_acc': 100. * val_correct / val_total\n",
        "        })\n",
        "\n",
        "    avg_train_loss = sum(f['train_loss'] for f in fold_results) / k\n",
        "    avg_train_acc = sum(f['train_acc'] for f in fold_results) / k\n",
        "    avg_val_loss = sum(f['val_loss'] for f in fold_results) / k\n",
        "    avg_val_acc = sum(f['val_acc'] for f in fold_results) / k\n",
        "\n",
        "    print(f\"\\nAverage Results after {k} folds:\")\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_acc:.2f}%\")\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_acc:.2f}%\")\n",
        "\n",
        "    return avg_train_loss, avg_train_acc"
      ],
      "metadata": {
        "id": "jOT5nCXnn2Gp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adversarial training"
      ],
      "metadata": {
        "id": "945tpA-j6ywB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def train_adversarial(model, dataset, optimizer, device, attack_type, epochs=10, k=5, patience=3, epsilon=8/255):\n",
        "    \"\"\"Adversarial training defense - Train on adversarial examples with K-Fold Cross-Validation and Early Stopping\"\"\"\n",
        "\n",
        "    # Hardcoded parameters for KFold and early stopping\n",
        "    kfold = KFold(n_splits=k, shuffle=True)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
        "        print(f'\\nFold {fold + 1}/{k}')\n",
        "\n",
        "        # Prepare the train and validation data loaders for this fold\n",
        "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
        "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
        "        train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=64, shuffle=False)\n",
        "\n",
        "        # Initialize early stopping parameters\n",
        "        best_val_loss = float('inf')\n",
        "        epochs_no_improve = 0\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        # Training loop for each fold\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs} [Adversarial Training]')\n",
        "            for images, labels in pbar:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                # Generate adversarial examples using attack\n",
        "                model.eval()  # Set to eval mode for attack generation\n",
        "                adv_images = None\n",
        "                if attack_type == 'pgd':\n",
        "                  adv_images = custom_pgd_attack(model, images, labels, epsilon=epsilon)\n",
        "                elif attack_type == 'deepfool':\n",
        "                  adv_images_list = []\n",
        "                  for i in range(images.size(0)):\n",
        "                    adv_images_list.append(custom_deepfool_attack(model, images[i]))\n",
        "                  adv_images = torch.stack(adv_images_list)\n",
        "                elif attack_type == 'none':\n",
        "                  adv_images = images\n",
        "                elif attack_type == 'both':\n",
        "                  adv_images_pgd = custom_pgd_attack(model, images, labels, epsilon=epsilon)\n",
        "                  adv_images_deepfool_list = []\n",
        "                  for i in range(images.size(0)):\n",
        "                    adv_images_deepfool_list.append(custom_deepfool_attack(model, images[i]))\n",
        "                  adv_images_deepfool = torch.stack(adv_images_deepfool_list)\n",
        "\n",
        "                  # Combine the adversarial images for random sampling\n",
        "                  # Convert tensors to lists of individual images, combine, then stack back\n",
        "                  combined_adv_images_list = adv_images_pgd.unbind(0) + adv_images_deepfool.unbind(0)\n",
        "                  selected_adv_images_list = random.sample(combined_adv_images_list, len(images))\n",
        "                  adv_images = torch.stack(selected_adv_images_list)\n",
        "                model.train()  # Back to train mode\n",
        "\n",
        "                # Train on adversarial examples\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(adv_images)\n",
        "                loss = F.cross_entropy(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                pbar.set_postfix({'loss': total_loss / total, 'acc': 100. * correct / total})\n",
        "\n",
        "            # Validation step after each epoch\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pbar_val = tqdm(val_loader, desc=f'Fold {fold + 1} [Validation]')\n",
        "                for images, labels in pbar_val:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                    # Perform validation on original images (not adversarial)\n",
        "                    outputs = model(images)\n",
        "                    loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                    pbar_val.set_postfix({'val_loss': val_loss / val_total, 'val_acc': 100. * val_correct / val_total})\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "            # Early stopping check\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                epochs_no_improve = 0\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())  # Save the best model weights\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            # If no improvement for 'patience' epochs, stop early\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                break\n",
        "\n",
        "        # Restore the best model weights after training\n",
        "        model.load_state_dict(best_model_wts)\n",
        "\n",
        "        # Store the results for this fold\n",
        "        fold_results.append({\n",
        "            'train_loss': total_loss / len(train_loader),\n",
        "            'train_acc': 100. * correct / total,\n",
        "        })\n",
        "\n",
        "    # Compute average results across all folds\n",
        "    avg_train_loss = sum(f['train_loss'] for f in fold_results) / k\n",
        "    avg_train_acc = sum(f['train_acc'] for f in fold_results) / k\n",
        "\n",
        "    print(f\"\\nAverage Results after {k} folds:\")\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_acc:.2f}%\")\n",
        "\n",
        "    return avg_train_loss, avg_train_acc"
      ],
      "metadata": {
        "id": "CV014hF6opAv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jacobian reg. training"
      ],
      "metadata": {
        "id": "s_IFwpYH649f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# K Fold training\n",
        "def train_jacobian(model, dataset, optimizer, device, epochs=10, k=5, patience=3, lambda_reg=1e-4): #lambda_reg 1e-4 1e-6\n",
        "    \"\"\"Train with Jacobian regularization defense with K-Fold Cross-Validation and Early Stopping\"\"\"\n",
        "\n",
        "    # Hardcoded parameters for KFold and early stopping\n",
        "    kfold = KFold(n_splits=k, shuffle=True)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
        "        print(f'\\nFold {fold + 1}/{k}')\n",
        "\n",
        "        # Prepare the train and validation data loaders for this fold\n",
        "        train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
        "        val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
        "        train_loader = torch.utils.data.DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "        val_loader = torch.utils.data.DataLoader(val_subset, batch_size=64, shuffle=False)\n",
        "\n",
        "        # Initialize early stopping parameters\n",
        "        best_val_loss = float('inf')\n",
        "        epochs_no_improve = 0\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        # Training loop for each fold\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs} [Jacobian Regularization]')\n",
        "            for images, labels in pbar:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss = compute_jacobian_regularization(model, images, labels, lambda_reg)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = model(images).max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                pbar.set_postfix({'loss': total_loss / total, 'acc': 100. * correct / total})\n",
        "\n",
        "            # Validation step after each epoch\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pbar_val = tqdm(val_loader, desc=f'Fold {fold + 1} [Validation]')\n",
        "                for images, labels in pbar_val:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                    # Perform validation on original images (not adversarial)\n",
        "                    outputs = model(images)\n",
        "                    loss = F.cross_entropy(outputs, labels)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    val_total += labels.size(0)\n",
        "                    val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                    pbar_val.set_postfix({'val_loss': val_loss / val_total, 'val_acc': 100. * val_correct / val_total})\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "            # Early stopping check\n",
        "            if avg_val_loss < best_val_loss:\n",
        "                best_val_loss = avg_val_loss\n",
        "                epochs_no_improve = 0\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())  # Save the best model weights\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "\n",
        "            # If no improvement for 'patience' epochs, stop early\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                break\n",
        "\n",
        "        # Restore the best model weights after training\n",
        "        model.load_state_dict(best_model_wts)\n",
        "\n",
        "        # Store the results for this fold\n",
        "        fold_results.append({\n",
        "            'train_loss': total_loss / len(train_loader),\n",
        "            'train_acc': 100. * correct / total,\n",
        "        })\n",
        "\n",
        "    # Compute average results across all folds\n",
        "    avg_train_loss = sum(f['train_loss'] for f in fold_results) / k\n",
        "    avg_train_acc = sum(f['train_acc'] for f in fold_results) / k\n",
        "\n",
        "    print(f\"\\nAverage Results after {k} folds:\")\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_acc:.2f}%\")\n",
        "\n",
        "    return avg_train_loss, avg_train_acc"
      ],
      "metadata": {
        "id": "6J4pYgJRo3Vn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "EN8AHKkW7Iy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def deepfool_batch(model, images):\n",
        "    # Helper function to apply custom_deepfool_attack to a batch of images\n",
        "    adv_images_list = []\n",
        "    for i in range(images.size(0)):\n",
        "        adv_images_list.append(custom_deepfool_attack(model, images[i]))\n",
        "    return torch.stack(adv_images_list)\n",
        "\n",
        "def evaluate(model, test_loader, device, attack_type=None, epsilon=8/255):\n",
        "    \"\"\"\n",
        "    Evaluate model on clean or adversarial examples\n",
        "\n",
        "    Args:\n",
        "        model: Neural network model\n",
        "        test_loader: Test data loader\n",
        "        device: Device to run on\n",
        "        attack_type: 'pgd', 'deepfool', or None for clean evaluation\n",
        "        epsilon: Attack strength\n",
        "\n",
        "    Returns:\n",
        "        Accuracy percentage, time taken\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    desc = f'Evaluating [{attack_type if attack_type else \"Clean\"}]'\n",
        "    pbar = tqdm(test_loader, desc=desc)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Generate adversarial examples if attack specified\n",
        "        if attack_type == 'pgd':\n",
        "            images = images.clone().detach().to(images.device)\n",
        "            labels = labels.clone().detach().to(labels.device)\n",
        "            images = custom_pgd_attack(model, images, labels, epsilon=epsilon)\n",
        "        elif attack_type == 'deepfool':\n",
        "            # Generate adversarial examples outside no_grad\n",
        "            images = deepfool_batch(model, images)\n",
        "\n",
        "        with torch.no_grad(): # Evaluate the model without gradient computation\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            pbar.set_postfix({'acc': 100.*correct/total})\n",
        "\n",
        "    end_time = time.time()\n",
        "    accuracy = 100. * correct / total\n",
        "    time_taken = end_time - start_time\n",
        "    return accuracy, time_taken"
      ],
      "metadata": {
        "id": "fwJ-saVJ7NCa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main experiment using implementations"
      ],
      "metadata": {
        "id": "njQxcOYg7OU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Load data\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train\n",
        ")\n",
        "test_dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "# Create model\n",
        "model = SimpleCNN().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxKwAZNH96IO",
        "outputId": "f46e2148-428a-4afc-b909-9ce4fe948ccd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 61.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count = 10 # Define the number of epochs for the k-fold training functions\n",
        "\n",
        "train_loss, train_acc = train_standard(model, train_dataset, optimizer, device, k=5, epochs=epoch_count, patience=3)\n",
        "print(f'Standard Training Complete: Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%')\n",
        "\n",
        "# Evaluate\n",
        "clean_acc = evaluate(model, test_loader, device, attack_type=None)\n",
        "pgd_acc = evaluate(model, test_loader, device, attack_type='pgd', epsilon=8/255)\n",
        "df_acc = evaluate(model, test_loader, device, attack_type='deepfool')\n",
        "\n",
        "print(f'Clean Accuracy: {clean_acc[0]:.2f}%')\n",
        "print(f'PGD Attack Accuracy: {pgd_acc[0]:.2f}%')\n",
        "print(f'DeepFool Attack Accuracy: {df_acc[0]:.2f}%') # Added print for DeepFool accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8s0-jkR7S99",
        "outputId": "580a8851-0cf4-4a1c-c220-d50f4e0ba554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 41.79it/s, loss=0.0111, acc=75.5]\n",
            "Fold 1 [Val]: 100%|██████████| 157/157 [00:03<00:00, 45.50it/s, val_loss=0.00977, val_acc=78.5]\n",
            "Epoch 2/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.14it/s, loss=0.0109, acc=75.9]\n",
            "Fold 1 [Val]: 100%|██████████| 157/157 [00:03<00:00, 51.23it/s, val_loss=0.00973, val_acc=78.4]\n",
            "Epoch 3/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.01it/s, loss=0.0106, acc=76]\n",
            "Fold 1 [Val]: 100%|██████████| 157/157 [00:03<00:00, 45.91it/s, val_loss=0.00983, val_acc=78.3]\n",
            "Epoch 4/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.37it/s, loss=0.0107, acc=76.4]\n",
            "Fold 1 [Val]: 100%|██████████| 157/157 [00:03<00:00, 51.26it/s, val_loss=0.0104, val_acc=76.9]\n",
            "Epoch 5/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.13it/s, loss=0.0107, acc=76.5]\n",
            "Fold 1 [Val]: 100%|██████████| 157/157 [00:03<00:00, 45.27it/s, val_loss=0.00989, val_acc=78.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 5\n",
            "\n",
            "Fold 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.35it/s, loss=0.011, acc=75.8]\n",
            "Fold 2 [Val]: 100%|██████████| 157/157 [00:03<00:00, 50.78it/s, val_loss=0.00968, val_acc=78.4]\n",
            "Epoch 2/10 [Train]: 100%|██████████| 625/625 [00:15<00:00, 41.30it/s, loss=0.0108, acc=76.2]\n",
            "Fold 2 [Val]: 100%|██████████| 157/157 [00:03<00:00, 45.56it/s, val_loss=0.00952, val_acc=78.5]\n",
            "Epoch 3/10 [Train]: 100%|██████████| 625/625 [00:15<00:00, 41.40it/s, loss=0.0106, acc=76.6]\n",
            "Fold 2 [Val]: 100%|██████████| 157/157 [00:03<00:00, 50.86it/s, val_loss=0.00945, val_acc=79.1]\n",
            "Epoch 4/10 [Train]: 100%|██████████| 625/625 [00:15<00:00, 40.09it/s, loss=0.0106, acc=76.7]\n",
            "Fold 2 [Val]: 100%|██████████| 157/157 [00:03<00:00, 47.99it/s, val_loss=0.00974, val_acc=77.9]\n",
            "Epoch 5/10 [Train]: 100%|██████████| 625/625 [00:15<00:00, 41.35it/s, loss=0.0105, acc=76.9]\n",
            "Fold 2 [Val]: 100%|██████████| 157/157 [00:03<00:00, 50.46it/s, val_loss=0.00983, val_acc=77.7]\n",
            "Epoch 6/10 [Train]: 100%|██████████| 625/625 [00:15<00:00, 40.54it/s, loss=0.0103, acc=77]\n",
            "Fold 2 [Val]: 100%|██████████| 157/157 [00:03<00:00, 51.38it/s, val_loss=0.00973, val_acc=78.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 6\n",
            "\n",
            "Fold 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 42.03it/s, loss=0.0106, acc=76.3]\n",
            "Fold 3 [Val]: 100%|██████████| 157/157 [00:03<00:00, 50.49it/s, val_loss=0.00909, val_acc=79.7]\n",
            "Epoch 2/10 [Train]: 100%|██████████| 625/625 [00:15<00:00, 39.85it/s, loss=0.0106, acc=76.5]\n",
            "Fold 3 [Val]: 100%|██████████| 157/157 [00:03<00:00, 50.81it/s, val_loss=0.00905, val_acc=80]\n",
            "Epoch 3/10 [Train]: 100%|██████████| 625/625 [00:15<00:00, 41.56it/s, loss=0.0104, acc=76.9]\n",
            "Fold 3 [Val]: 100%|██████████| 157/157 [00:03<00:00, 48.67it/s, val_loss=0.00924, val_acc=79.5]\n",
            "Epoch 4/10 [Train]: 100%|██████████| 625/625 [00:15<00:00, 40.09it/s, loss=0.0105, acc=77]\n",
            "Fold 3 [Val]: 100%|██████████| 157/157 [00:03<00:00, 50.70it/s, val_loss=0.00949, val_acc=78.8]\n",
            "Epoch 5/10 [Train]: 100%|██████████| 625/625 [00:15<00:00, 41.35it/s, loss=0.0103, acc=77.2]\n",
            "Fold 3 [Val]: 100%|██████████| 157/157 [00:03<00:00, 47.06it/s, val_loss=0.00952, val_acc=78.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 5\n",
            "\n",
            "Fold 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 625/625 [00:15<00:00, 40.64it/s, loss=0.0105, acc=77]\n",
            "Fold 4 [Val]: 100%|██████████| 157/157 [00:03<00:00, 50.46it/s, val_loss=0.00911, val_acc=79.3]\n",
            "Epoch 2/10 [Train]: 100%|██████████| 625/625 [00:15<00:00, 41.44it/s, loss=0.0104, acc=77.1]\n",
            "Fold 4 [Val]: 100%|██████████| 157/157 [00:03<00:00, 44.75it/s, val_loss=0.00922, val_acc=79.7]\n",
            "Epoch 3/10 [Train]: 100%|██████████| 625/625 [00:15<00:00, 41.20it/s, loss=0.0103, acc=77.3]\n",
            "Fold 4 [Val]: 100%|██████████| 157/157 [00:03<00:00, 50.63it/s, val_loss=0.00918, val_acc=79.9]\n",
            "Epoch 4/10 [Train]: 100%|██████████| 625/625 [00:15<00:00, 41.59it/s, loss=0.0102, acc=77.4]\n",
            "Fold 4 [Val]: 100%|██████████| 157/157 [00:03<00:00, 42.85it/s, val_loss=0.00922, val_acc=79.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 4\n",
            "\n",
            "Fold 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Train]: 100%|██████████| 625/625 [00:14<00:00, 41.84it/s, loss=0.0105, acc=77]\n",
            "Fold 5 [Val]: 100%|██████████| 157/157 [00:03<00:00, 50.62it/s, val_loss=0.0089, val_acc=80.3]\n",
            "Epoch 2/10 [Train]: 100%|██████████| 625/625 [00:15<00:00, 41.50it/s, loss=0.0104, acc=77.1]\n",
            "Fold 5 [Val]: 100%|██████████| 157/157 [00:03<00:00, 41.05it/s, val_loss=0.00905, val_acc=79.7]\n",
            "Epoch 3/10 [Train]: 100%|██████████| 625/625 [00:15<00:00, 41.39it/s, loss=0.0104, acc=76.9]\n",
            "Fold 5 [Val]: 100%|██████████| 157/157 [00:03<00:00, 49.44it/s, val_loss=0.00945, val_acc=79]\n",
            "Epoch 4/10 [Train]: 100%|██████████| 625/625 [00:15<00:00, 41.56it/s, loss=0.0102, acc=77.5]\n",
            "Fold 5 [Val]: 100%|██████████| 157/157 [00:03<00:00, 41.75it/s, val_loss=0.00892, val_acc=80]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 4\n",
            "\n",
            "Average Results after 5 folds:\n",
            "Train Loss: 0.6624, Train Accuracy: 77.13%\n",
            "Validation Loss: 0.6023, Validation Accuracy: 78.85%\n",
            "Standard Training Complete: Loss=0.6624, Train Acc=77.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating [Clean]: 100%|██████████| 79/79 [00:01<00:00, 51.13it/s, acc=79.2]\n",
            "Evaluating [pgd]: 100%|██████████| 79/79 [00:04<00:00, 18.12it/s, acc=0.01]\n",
            "Evaluating [deepfool]: 100%|██████████| 79/79 [13:25<00:00, 10.19s/it, acc=11.4]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean Accuracy: 79.23%\n",
            "PGD Attack Accuracy: 0.01%\n",
            "DeepFool Attack Accuracy: 11.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new model\n",
        "model_adv = SimpleCNN().to(device)\n",
        "optimizer_adv = torch.optim.Adam(model_adv.parameters(), lr=0.001)\n",
        "\n",
        "# Train with adversarial examples\n",
        "epoch_count = 10 # Number of epochs for the k-fold training\n",
        "train_loss, train_acc = train_adversarial(model_adv, train_dataset, optimizer_adv, device, attack_type='pgd', k=5, epochs=epoch_count, patience=3, epsilon=8/255)\n",
        "print(f'Adversarial Training Complete: Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%')\n",
        "\n",
        "# Evaluate\n",
        "clean_acc = evaluate(model_adv, test_loader, device, attack_type=None)\n",
        "pgd_acc = evaluate(model_adv, test_loader, device, attack_type='pgd', epsilon=8/255)\n",
        "df_acc = evaluate(model_adv, test_loader, device, attack_type='deepfool') # Changed model to model_adv\n",
        "\n",
        "print(f'Clean Accuracy: {clean_acc[0]:.2f}%')\n",
        "print(f'PGD Attack Accuracy: {pgd_acc[0]:.2f}%')\n",
        "print(f'DeepFool Attack Accuracy: {df_acc[0]:.2f}%') # Added print for DeepFool accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-plGkIrhTtMd",
        "outputId": "4d3096e0-75da-45a0-ea15-ed55b38526af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.59it/s, loss=0.0344, acc=17.8]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 50.79it/s, val_loss=0.0304, val_acc=32.3]\n",
            "Epoch 2/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.75it/s, loss=0.0328, acc=22.7]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 52.11it/s, val_loss=0.028, val_acc=37.4]\n",
            "Epoch 3/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 20.89it/s, loss=0.0321, acc=24.2]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 42.60it/s, val_loss=0.028, val_acc=37.3]\n",
            "Epoch 4/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 20.88it/s, loss=0.0318, acc=24.9]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 48.70it/s, val_loss=0.0275, val_acc=39]\n",
            "Epoch 5/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.73it/s, loss=0.0316, acc=25.2]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 50.75it/s, val_loss=0.0269, val_acc=40.1]\n",
            "Epoch 6/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.68it/s, loss=0.0315, acc=25.6]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 51.46it/s, val_loss=0.0265, val_acc=39.8]\n",
            "Epoch 7/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 20.84it/s, loss=0.0313, acc=26]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 44.16it/s, val_loss=0.0266, val_acc=41]\n",
            "Epoch 8/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 21.01it/s, loss=0.0312, acc=26]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 46.82it/s, val_loss=0.0263, val_acc=40.8]\n",
            "Epoch 9/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.73it/s, loss=0.031, acc=26.4]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 50.54it/s, val_loss=0.026, val_acc=42.1]\n",
            "Epoch 10/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.67it/s, loss=0.031, acc=26.5]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 50.78it/s, val_loss=0.0259, val_acc=40.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.70it/s, loss=0.0308, acc=26.8]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 47.13it/s, val_loss=0.0263, val_acc=42.2]\n",
            "Epoch 2/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.82it/s, loss=0.0307, acc=26.7]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 42.82it/s, val_loss=0.0257, val_acc=43.5]\n",
            "Epoch 3/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.83it/s, loss=0.0307, acc=26.9]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 50.57it/s, val_loss=0.0258, val_acc=40.8]\n",
            "Epoch 4/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.48it/s, loss=0.0306, acc=27.1]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 49.84it/s, val_loss=0.0256, val_acc=43.4]\n",
            "Epoch 5/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.32it/s, loss=0.0306, acc=27.3]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 51.43it/s, val_loss=0.0257, val_acc=43.7]\n",
            "Epoch 6/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.81it/s, loss=0.0305, acc=27.3]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 42.13it/s, val_loss=0.0253, val_acc=42.5]\n",
            "Epoch 7/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.79it/s, loss=0.0305, acc=27.3]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 48.23it/s, val_loss=0.0252, val_acc=44]\n",
            "Epoch 8/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.18it/s, loss=0.0305, acc=27.6]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 39.62it/s, val_loss=0.0252, val_acc=42.5]\n",
            "Epoch 9/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.64it/s, loss=0.0304, acc=27.7]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 50.62it/s, val_loss=0.0259, val_acc=43.5]\n",
            "Epoch 10/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.48it/s, loss=0.0304, acc=27.9]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 49.88it/s, val_loss=0.0251, val_acc=43.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.82it/s, loss=0.0304, acc=27.4]\n",
            "Fold 3 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 42.66it/s, val_loss=0.0254, val_acc=44.7]\n",
            "Epoch 2/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.67it/s, loss=0.0305, acc=27.5]\n",
            "Fold 3 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 49.53it/s, val_loss=0.0249, val_acc=45.6]\n",
            "Epoch 3/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.57it/s, loss=0.0304, acc=27.7]\n",
            "Fold 3 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 51.25it/s, val_loss=0.0251, val_acc=44.9]\n",
            "Epoch 4/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.61it/s, loss=0.0304, acc=27.6]\n",
            "Fold 3 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 51.54it/s, val_loss=0.0252, val_acc=44.3]\n",
            "Epoch 5/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 20.95it/s, loss=0.0304, acc=27.7]\n",
            "Fold 3 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 43.48it/s, val_loss=0.0248, val_acc=46]\n",
            "Epoch 6/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 20.92it/s, loss=0.0303, acc=27.9]\n",
            "Fold 3 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 47.47it/s, val_loss=0.0256, val_acc=43.6]\n",
            "Epoch 7/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.73it/s, loss=0.0303, acc=27.7]\n",
            "Fold 3 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 50.88it/s, val_loss=0.0251, val_acc=45.6]\n",
            "Epoch 8/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.63it/s, loss=0.0302, acc=28.1]\n",
            "Fold 3 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 51.74it/s, val_loss=0.0247, val_acc=44.7]\n",
            "Epoch 9/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.79it/s, loss=0.0303, acc=27.9]\n",
            "Fold 3 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 45.93it/s, val_loss=0.0251, val_acc=46.6]\n",
            "Epoch 10/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 21.00it/s, loss=0.0303, acc=27.9]\n",
            "Fold 3 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 45.53it/s, val_loss=0.0252, val_acc=46.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.64it/s, loss=0.0303, acc=27.9]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 50.59it/s, val_loss=0.0249, val_acc=46]\n",
            "Epoch 2/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.54it/s, loss=0.0303, acc=28]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 51.30it/s, val_loss=0.025, val_acc=45.4]\n",
            "Epoch 3/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.63it/s, loss=0.0302, acc=27.9]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 50.86it/s, val_loss=0.0249, val_acc=44.8]\n",
            "Epoch 4/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.83it/s, loss=0.0302, acc=27.9]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 42.80it/s, val_loss=0.0248, val_acc=45.6]\n",
            "Epoch 5/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.82it/s, loss=0.0302, acc=28]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 51.89it/s, val_loss=0.0247, val_acc=45]\n",
            "Epoch 6/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.55it/s, loss=0.0302, acc=28]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 50.54it/s, val_loss=0.025, val_acc=46.7]\n",
            "Epoch 7/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.56it/s, loss=0.0302, acc=28]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 51.19it/s, val_loss=0.0248, val_acc=46.5]\n",
            "Epoch 8/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 20.97it/s, loss=0.0302, acc=28.1]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 42.71it/s, val_loss=0.0246, val_acc=45.4]\n",
            "Epoch 9/10 [Adversarial Training]: 100%|██████████| 625/625 [00:29<00:00, 20.96it/s, loss=0.0302, acc=28.1]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 52.33it/s, val_loss=0.0246, val_acc=46.8]\n",
            "Epoch 10/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.30it/s, loss=0.0302, acc=28.1]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 49.92it/s, val_loss=0.0248, val_acc=46.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.50it/s, loss=0.0301, acc=27.9]\n",
            "Fold 5 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 49.53it/s, val_loss=0.0247, val_acc=46]\n",
            "Epoch 2/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.73it/s, loss=0.0301, acc=28.1]\n",
            "Fold 5 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 43.61it/s, val_loss=0.0249, val_acc=45.1]\n",
            "Epoch 3/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.80it/s, loss=0.0301, acc=28.3]\n",
            "Fold 5 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 45.53it/s, val_loss=0.025, val_acc=46.1]\n",
            "Epoch 4/10 [Adversarial Training]: 100%|██████████| 625/625 [00:30<00:00, 20.66it/s, loss=0.0301, acc=28.1]\n",
            "Fold 5 [Validation]: 100%|██████████| 157/157 [00:03<00:00, 51.42it/s, val_loss=0.0248, val_acc=44.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 4\n",
            "\n",
            "Average Results after 5 folds:\n",
            "Train Loss: 1.9452, Train Accuracy: 27.69%\n",
            "Adversarial Training Complete: Loss=1.9452, Train Acc=27.69%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating [Clean]: 100%|██████████| 79/79 [00:01<00:00, 52.04it/s, acc=48.3]\n",
            "Evaluating [pgd]: 100%|██████████| 79/79 [00:04<00:00, 17.33it/s, acc=30.7]\n",
            "Evaluating [deepfool]: 100%|██████████| 79/79 [35:04<00:00, 26.64s/it, acc=21.2]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean Accuracy: 48.30%\n",
            "PGD Attack Accuracy: 30.66%\n",
            "DeepFool Attack Accuracy: 21.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new model\n",
        "model_jac = SimpleCNN().to(device)\n",
        "optimizer_jac = torch.optim.Adam(model_jac.parameters(), lr=0.001)\n",
        "\n",
        "# Train with Jacobian regularization\n",
        "epoch_count = 10 # Number of epochs for the k-fold training\n",
        "train_loss, train_acc = train_jacobian(model_jac, train_dataset, optimizer_jac, device, k=5, epochs=epoch_count, patience=3, lambda_reg=1e-4)\n",
        "print(f'Jacobian Regularization Training Complete: Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%')\n",
        "\n",
        "# Evaluate\n",
        "clean_acc = evaluate(model_jac, test_loader, device, attack_type=None)\n",
        "pgd_acc = evaluate(model_jac, test_loader, device, attack_type='pgd', epsilon=8/255)\n",
        "df_acc = evaluate(model_jac, test_loader, device, attack_type='deepfool') # Changed model to model_jac\n",
        "\n",
        "print(f'Clean Accuracy: {clean_acc[0]:.2f}%')\n",
        "print(f'PGD Attack Accuracy: {pgd_acc[0]:.2f}%')\n",
        "print(f'DeepFool Attack Accuracy: {df_acc[0]:.2f}%') # Added print for DeepFool accuracy"
      ],
      "metadata": {
        "id": "8j6qAH28TuVU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9798be6-347a-491f-9ac1-966236389a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Jacobian Regularization]: 100%|██████████| 625/625 [09:57<00:00,  1.05it/s, loss=4.83e+15, acc=9.82]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:11<00:00, 13.47it/s, val_loss=1.38e+8, val_acc=10.3]\n",
            "Epoch 2/10 [Jacobian Regularization]: 100%|██████████| 625/625 [10:09<00:00,  1.02it/s, loss=1.11e+19, acc=10.1]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:11<00:00, 13.18it/s, val_loss=2.22e+9, val_acc=10.3]\n",
            "Epoch 3/10 [Jacobian Regularization]: 100%|██████████| 625/625 [10:03<00:00,  1.03it/s, loss=7.07e+20, acc=9.91]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:11<00:00, 13.23it/s, val_loss=9.71e+9, val_acc=10.3]\n",
            "Epoch 4/10 [Jacobian Regularization]: 100%|██████████| 625/625 [10:04<00:00,  1.03it/s, loss=1.12e+22, acc=9.83]\n",
            "Fold 1 [Validation]: 100%|██████████| 157/157 [00:12<00:00, 13.04it/s, val_loss=2.62e+10, val_acc=10.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 4\n",
            "\n",
            "Fold 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Jacobian Regularization]: 100%|██████████| 625/625 [09:59<00:00,  1.04it/s, loss=6.56e+16, acc=9.75]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:11<00:00, 13.52it/s, val_loss=1.48e+8, val_acc=9.88]\n",
            "Epoch 2/10 [Jacobian Regularization]: 100%|██████████| 625/625 [10:00<00:00,  1.04it/s, loss=6.99e+16, acc=9.96]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:11<00:00, 13.16it/s, val_loss=1.52e+8, val_acc=9.88]\n",
            "Epoch 3/10 [Jacobian Regularization]: 100%|██████████| 625/625 [10:01<00:00,  1.04it/s, loss=7.63e+16, acc=10.1]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:11<00:00, 13.22it/s, val_loss=1.59e+8, val_acc=9.88]\n",
            "Epoch 4/10 [Jacobian Regularization]: 100%|██████████| 625/625 [10:00<00:00,  1.04it/s, loss=8.64e+16, acc=9.86]\n",
            "Fold 2 [Validation]: 100%|██████████| 157/157 [00:11<00:00, 13.15it/s, val_loss=1.69e+8, val_acc=9.88]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 4\n",
            "\n",
            "Fold 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Jacobian Regularization]: 100%|██████████| 625/625 [10:00<00:00,  1.04it/s, loss=7.4e+16, acc=10.1]\n",
            "Fold 3 [Validation]: 100%|██████████| 157/157 [00:11<00:00, 13.19it/s, val_loss=1.6e+8, val_acc=9.68]\n",
            "Epoch 2/10 [Jacobian Regularization]: 100%|██████████| 625/625 [10:00<00:00,  1.04it/s, loss=9.35e+16, acc=9.99]\n",
            "Fold 3 [Validation]: 100%|██████████| 157/157 [00:11<00:00, 13.55it/s, val_loss=1.8e+8, val_acc=9.68]\n",
            "Epoch 3/10 [Jacobian Regularization]: 100%|██████████| 625/625 [09:59<00:00,  1.04it/s, loss=1.33e+17, acc=10]\n",
            "Fold 3 [Validation]: 100%|██████████| 157/157 [00:11<00:00, 13.25it/s, val_loss=2.14e+8, val_acc=9.68]\n",
            "Epoch 4/10 [Jacobian Regularization]: 100%|██████████| 625/625 [10:01<00:00,  1.04it/s, loss=2.31e+17, acc=10.2]\n",
            "Fold 3 [Validation]: 100%|██████████| 157/157 [00:11<00:00, 13.31it/s, val_loss=2.81e+8, val_acc=9.68]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 4\n",
            "\n",
            "Fold 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Jacobian Regularization]: 100%|██████████| 625/625 [09:59<00:00,  1.04it/s, loss=1.19e+17, acc=9.88]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:11<00:00, 13.19it/s, val_loss=2.16e+8, val_acc=9.96]\n",
            "Epoch 2/10 [Jacobian Regularization]: 100%|██████████| 625/625 [10:00<00:00,  1.04it/s, loss=3.48e+17, acc=10.1]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:12<00:00, 13.02it/s, val_loss=3.75e+8, val_acc=9.96]\n",
            "Epoch 3/10 [Jacobian Regularization]: 100%|██████████| 625/625 [10:02<00:00,  1.04it/s, loss=3.42e+18, acc=9.99]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:11<00:00, 13.14it/s, val_loss=1.2e+9, val_acc=9.96]\n",
            "Epoch 4/10 [Jacobian Regularization]: 100%|██████████| 625/625 [10:03<00:00,  1.04it/s, loss=2.27e+20, acc=9.93]\n",
            "Fold 4 [Validation]: 100%|██████████| 157/157 [00:11<00:00, 13.45it/s, val_loss=6.85e+9, val_acc=9.96]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping at epoch 4\n",
            "\n",
            "Fold 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10 [Jacobian Regularization]: 100%|██████████| 625/625 [10:03<00:00,  1.04it/s, loss=2.37e+17, acc=9.96]\n",
            "Fold 5 [Validation]: 100%|██████████| 157/157 [00:11<00:00, 13.12it/s, val_loss=2.66e+8, val_acc=10.2]\n",
            "Epoch 2/10 [Jacobian Regularization]: 100%|██████████| 625/625 [10:03<00:00,  1.04it/s, loss=3.88e+17, acc=10.1]\n",
            "Fold 5 [Validation]: 100%|██████████| 157/157 [00:12<00:00, 13.08it/s, val_loss=3.64e+8, val_acc=10.2]\n",
            "Epoch 3/10 [Jacobian Regularization]: 100%|██████████| 625/625 [10:04<00:00,  1.03it/s, loss=8.86e+17, acc=9.86]\n",
            "Fold 5 [Validation]: 100%|██████████| 157/157 [00:11<00:00, 13.16it/s, val_loss=5.53e+8, val_acc=10.2]\n",
            "Epoch 4/10 [Jacobian Regularization]:  49%|████▉     | 306/625 [04:54<04:50,  1.10it/s, loss=2.21e+18, acc=9.93]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "# Test PGD attack\n",
        "adv_images_pgd = custom_pgd_attack(model, images, labels, epsilon=8/255)\n",
        "\n",
        "# Test DeepFool attack (on single image)\n",
        "single_image = images[0]\n",
        "adv_image_deepfool = custom_deepfool_attack(model, single_image)\n",
        "\n",
        "# Compare predictions\n",
        "with torch.no_grad():\n",
        "    clean_pred = model(images).argmax(dim=1)\n",
        "    pgd_pred = model(adv_images_pgd).argmax(dim=1)\n",
        "\n",
        "print(\"Clean predictions:\", clean_pred[:10])\n",
        "print(\"PGD adversarial predictions:\", pgd_pred[:10])\n",
        "print(\"True labels:\", labels[:10])"
      ],
      "metadata": {
        "id": "HFJ5FzkmTyTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get one image\n",
        "img = images[0].cpu()\n",
        "adv_img = adv_images_pgd[0].cpu()\n",
        "\n",
        "# Calculate perturbation\n",
        "perturbation = (adv_img - img).abs()\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "axes[0].imshow(img.permute(1, 2, 0))\n",
        "axes[0].set_title('Clean Image')\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(adv_img.permute(1, 2, 0))\n",
        "axes[1].set_title('Adversarial Image')\n",
        "axes[1].axis('off')\n",
        "\n",
        "axes[2].imshow(perturbation.permute(1, 2, 0) * 10)  # Amplified for visibility\n",
        "axes[2].set_title('Perturbation (10x)')\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Max perturbation: {perturbation.max():.4f}\")\n",
        "print(f\"L2 norm: {perturbation.norm():.4f}\")"
      ],
      "metadata": {
        "id": "1TbHcqU3T1Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkNFYK4vsTPK"
      },
      "source": [
        "# State of the art implementations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install adversarial-robustness-toolbox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu9aEbbuY4CJ",
        "outputId": "65c19143-c2b3-4856-f086-2aafd37e2e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adversarial-robustness-toolbox\n",
            "  Downloading adversarial_robustness_toolbox-1.20.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (1.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (1.17.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (75.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from adversarial-robustness-toolbox) (4.67.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2->adversarial-robustness-toolbox) (3.6.0)\n",
            "Downloading adversarial_robustness_toolbox-1.20.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: adversarial-robustness-toolbox\n",
            "Successfully installed adversarial-robustness-toolbox-1.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# ART imports\n",
        "from art.estimators.classification import PyTorchClassifier\n",
        "from art.attacks.evasion import ProjectedGradientDescent, DeepFool"
      ],
      "metadata": {
        "id": "-SEH0W2Sbiv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparisson to custom implementation"
      ],
      "metadata": {
        "id": "w3bo3ty-Y0O4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_custom_pgd(model, test_loader, device, epsilon=8/255, max_samples=1000):\n",
        "    \"\"\"Evaluate using custom PGD implementation\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    times = []\n",
        "\n",
        "    for images, labels in tqdm(test_loader, desc='Custom PGD'):\n",
        "        if total >= max_samples:\n",
        "            break\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        start_time = time.time()\n",
        "        adv_images = custom_pgd_attack(model, images, labels, epsilon=epsilon)\n",
        "        times.append(time.time() - start_time)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(adv_images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    avg_time = np.mean(times)\n",
        "    return accuracy, avg_time\n",
        "\n",
        "\n",
        "def evaluate_art_pgd(classifier, test_loader, epsilon=8/255, max_samples=1000):\n",
        "    \"\"\"Evaluate using ART PGD implementation\"\"\"\n",
        "    pgd_attack = ProjectedGradientDescent(\n",
        "        estimator=classifier,\n",
        "        eps=epsilon,\n",
        "        eps_step=epsilon/4,\n",
        "        max_iter=10,\n",
        "        num_random_init=1,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    times = []\n",
        "\n",
        "    for images, labels in tqdm(test_loader, desc='ART PGD'):\n",
        "        if total >= max_samples:\n",
        "            break\n",
        "\n",
        "        x_batch = images.numpy()\n",
        "        y_batch = labels.numpy()\n",
        "\n",
        "        start_time = time.time()\n",
        "        x_adv = pgd_attack.generate(x=x_batch)\n",
        "        times.append(time.time() - start_time)\n",
        "\n",
        "        predictions = classifier.predict(x_adv)\n",
        "        predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "        total += len(y_batch)\n",
        "        correct += np.sum(predicted_labels == y_batch)\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    avg_time = np.mean(times)\n",
        "    return accuracy, avg_time\n",
        "\n",
        "\n",
        "def calculate_perturbation_metrics(model, test_loader, device, attack_fn, epsilon=8/255, max_samples=100):\n",
        "    \"\"\"Calculate L2 and L-inf norms of perturbations\"\"\"\n",
        "    model.eval()\n",
        "    l2_norms = []\n",
        "    linf_norms = []\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        if len(l2_norms) >= max_samples:\n",
        "            break\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        if 'custom' in attack_fn.__name__:\n",
        "            adv_images = attack_fn(model, images, labels, epsilon=epsilon)\n",
        "        else:\n",
        "            # For DeepFool\n",
        "            for i in range(images.size(0)):\n",
        "                if len(l2_norms) >= max_samples:\n",
        "                    break\n",
        "                adv_img = attack_fn(model, images[i])\n",
        "                perturbation = (adv_img - images[i]).abs()\n",
        "                l2_norms.append(perturbation.norm(p=2).item())\n",
        "                linf_norms.append(perturbation.max().item())\n",
        "            continue\n",
        "\n",
        "        perturbation = (adv_images - images).abs()\n",
        "        for i in range(perturbation.size(0)):\n",
        "            if len(l2_norms) >= max_samples:\n",
        "                break\n",
        "            l2_norms.append(perturbation[i].norm(p=2).item())\n",
        "            linf_norms.append(perturbation[i].max().item())\n",
        "\n",
        "    return {\n",
        "        'l2_mean': np.mean(l2_norms),\n",
        "        'l2_std': np.std(l2_norms),\n",
        "        'linf_mean': np.mean(linf_norms),\n",
        "        'linf_std': np.std(linf_norms)\n",
        "    }"
      ],
      "metadata": {
        "id": "-yl6RqYQFZz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_custom_deepfool(model, test_loader, device, max_samples=200):\n",
        "    \"\"\"Evaluate using custom DeepFool implementation\"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    times = []\n",
        "\n",
        "    for images, labels in tqdm(test_loader, desc='Custom DeepFool'):\n",
        "        if total >= max_samples:\n",
        "            break\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        for i in range(images.size(0)):\n",
        "            if total >= max_samples:\n",
        "                break\n",
        "\n",
        "            start_time = time.time()\n",
        "            adv_img = custom_deepfool_attack(model, images[i])\n",
        "            times.append(time.time() - start_time)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output = model(adv_img.unsqueeze(0))\n",
        "                predicted = output.argmax(dim=1).item()\n",
        "                total += 1\n",
        "                if predicted == labels[i].item():\n",
        "                    correct += 1\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    avg_time = np.mean(times)\n",
        "    return accuracy, avg_time\n",
        "\n",
        "\n",
        "def evaluate_art_deepfool(classifier, test_loader, max_samples=200):\n",
        "    \"\"\"Evaluate using ART DeepFool implementation\"\"\"\n",
        "    deepfool_attack = DeepFool(\n",
        "        classifier=classifier,\n",
        "        max_iter=50,\n",
        "        epsilon=1e-6,\n",
        "        nb_grads=10,\n",
        "        batch_size=1,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    times = []\n",
        "\n",
        "    for images, labels in tqdm(test_loader, desc='ART DeepFool'):\n",
        "        if total >= max_samples:\n",
        "            break\n",
        "\n",
        "        x_batch = images.numpy()\n",
        "        y_batch = labels.numpy()\n",
        "\n",
        "        start_time = time.time()\n",
        "        x_adv = deepfool_attack.generate(x=x_batch)\n",
        "        times.append(time.time() - start_time)\n",
        "\n",
        "        predictions = classifier.predict(x_adv)\n",
        "        predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "        total += len(y_batch)\n",
        "        correct += np.sum(predicted_labels == y_batch)\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    avg_time = np.mean(times)\n",
        "    return accuracy, avg_time"
      ],
      "metadata": {
        "id": "Idbg3pAsxwbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"=\"*70)\n",
        "    print(\"CUSTOM vs STATE-OF-THE-ART ADVERSARIAL ML COMPARISON\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f'\\nUsing device: {device}')\n",
        "\n",
        "    # Load data\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=False, download=True, transform=transform\n",
        "    )\n",
        "    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Load or train a simple model\n",
        "    print(\"\\nTraining a simple model for comparison...\")\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root='./data', train=True, download=True, transform=transform\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "    model = SimpleCNN().to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Quick training (3 epochs)\n",
        "    for epoch in range(1, 4):\n",
        "        model.train()\n",
        "        for images, labels in tqdm(train_loader, desc=f'Training Epoch {epoch}'):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluate clean accuracy\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    clean_acc = 100. * correct / total\n",
        "    print(f\"\\nClean Accuracy: {clean_acc:.2f}%\")\n",
        "\n",
        "    # Wrap model in ART classifier\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    classifier = PyTorchClassifier(\n",
        "        model=model,\n",
        "        clip_values=(0, 1),\n",
        "        loss=criterion,\n",
        "        optimizer=optimizer,\n",
        "        input_shape=(3, 32, 32),\n",
        "        nb_classes=10,\n",
        "    )\n",
        "\n",
        "    epsilon = 8/255\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PGD ATTACK COMPARISON\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Custom PGD\n",
        "    print(\"\\n[1/2] Evaluating Custom PGD...\")\n",
        "    custom_pgd_acc, custom_pgd_time = evaluate_custom_pgd(\n",
        "        model, test_loader, device, epsilon=epsilon, max_samples=1000\n",
        "    )\n",
        "\n",
        "    # ART PGD\n",
        "    print(\"\\n[2/2] Evaluating ART PGD...\")\n",
        "    art_pgd_acc, art_pgd_time = evaluate_art_pgd(\n",
        "        classifier, test_loader, epsilon=epsilon, max_samples=1000\n",
        "    )\n",
        "\n",
        "    # Calculate perturbation metrics for PGD\n",
        "    print(\"\\nCalculating perturbation metrics for PGD...\")\n",
        "    custom_pgd_metrics = calculate_perturbation_metrics(\n",
        "        model, test_loader, device, custom_pgd_attack, epsilon=epsilon, max_samples=100\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"PGD RESULTS:\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"{'Metric':<30} {'Custom':<20} {'ART (SOTA)':<20}\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"{'Accuracy':<30} {custom_pgd_acc:<20.2f} {art_pgd_acc:<20.2f}\")\n",
        "    print(f\"{'Avg Time per Batch (s)':<30} {custom_pgd_time:<20.4f} {art_pgd_time:<20.4f}\")\n",
        "    print(f\"{'L2 Norm (mean)':<30} {custom_pgd_metrics['l2_mean']:<20.4f} {'N/A':<20}\")\n",
        "    print(f\"{'L-inf Norm (mean)':<30} {custom_pgd_metrics['linf_mean']:<20.4f} {'N/A':<20}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    improvement_acc = art_pgd_acc - custom_pgd_acc\n",
        "    improvement_time = ((custom_pgd_time - art_pgd_time) / custom_pgd_time) * 100\n",
        "\n",
        "    print(f\"\\nART vs Custom:\")\n",
        "    print(f\"  Accuracy difference: {improvement_acc:+.2f}%\")\n",
        "    print(f\"  Speed improvement: {improvement_time:+.1f}%\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"DEEPFOOL ATTACK COMPARISON\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Custom DeepFool\n",
        "    print(\"\\n[1/2] Evaluating Custom DeepFool...\")\n",
        "    custom_df_acc, custom_df_time = evaluate_custom_deepfool(\n",
        "        model, test_loader, device, max_samples=200\n",
        "    )\n",
        "\n",
        "    # ART DeepFool\n",
        "    print(\"\\n[2/2] Evaluating ART DeepFool...\")\n",
        "    art_df_acc, art_df_time = evaluate_art_deepfool(\n",
        "        classifier, test_loader, max_samples=200\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"-\"*70)\n",
        "    print(\"DEEPFOOL RESULTS:\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"{'Metric':<30} {'Custom':<20} {'ART (SOTA)':<20}\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"{'Accuracy':<30} {custom_df_acc:<20.2f} {art_df_acc:<20.2f}\")\n",
        "    print(f\"{'Avg Time per Sample (s)':<30} {custom_df_time:<20.4f} {art_df_time:<20.4f}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    improvement_acc_df = art_df_acc - custom_df_acc\n",
        "    improvement_time_df = ((custom_df_time - art_df_time) / custom_df_time) * 100\n",
        "\n",
        "    print(f\"\\nART vs Custom:\")\n",
        "    print(f\"  Accuracy difference: {improvement_acc_df:+.2f}%\")\n",
        "    print(f\"  Speed improvement: {improvement_time_df:+.1f}%\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nClean Accuracy: {clean_acc:.2f}%\")\n",
        "    print(f\"\\nPGD Attack:\")\n",
        "    print(f\"  Custom:    {custom_pgd_acc:.2f}% (defender wins if higher)\")\n",
        "    print(f\"  ART SOTA:  {art_pgd_acc:.2f}%\")\n",
        "    print(f\"\\nDeepFool Attack:\")\n",
        "    print(f\"  Custom:    {custom_df_acc:.2f}%\")\n",
        "    print(f\"  ART SOTA:  {art_df_acc:.2f}%\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "1pqGPDV2Fght",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f54fae2-a4a1-4025-e92d-add7cde0a08c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CUSTOM vs STATE-OF-THE-ART ADVERSARIAL ML COMPARISON\n",
            "======================================================================\n",
            "\n",
            "Using device: cuda\n",
            "\n",
            "Training a simple model for comparison...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 391/391 [00:07<00:00, 50.04it/s]\n",
            "Training Epoch 2: 100%|██████████| 391/391 [00:06<00:00, 56.27it/s]\n",
            "Training Epoch 3: 100%|██████████| 391/391 [00:08<00:00, 44.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Clean Accuracy: 61.79%\n",
            "\n",
            "======================================================================\n",
            "PGD ATTACK COMPARISON\n",
            "======================================================================\n",
            "\n",
            "[1/2] Evaluating Custom PGD...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Custom PGD:  10%|█         | 10/100 [00:00<00:05, 15.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2/2] Evaluating ART PGD...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ART PGD:  10%|█         | 10/100 [00:02<00:20,  4.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Calculating perturbation metrics for PGD...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "PGD RESULTS:\n",
            "----------------------------------------------------------------------\n",
            "Metric                         Custom               ART (SOTA)          \n",
            "----------------------------------------------------------------------\n",
            "Accuracy                       2.90                 13.40               \n",
            "Avg Time per Batch (s)         0.0305               0.2079              \n",
            "L2 Norm (mean)                 1.5258               N/A                 \n",
            "L-inf Norm (mean)              0.0314               N/A                 \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "ART vs Custom:\n",
            "  Accuracy difference: +10.50%\n",
            "  Speed improvement: -580.8%\n",
            "\n",
            "======================================================================\n",
            "DEEPFOOL ATTACK COMPARISON\n",
            "======================================================================\n",
            "\n",
            "[1/2] Evaluating Custom DeepFool...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Custom DeepFool:   2%|▏         | 2/100 [00:25<20:33, 12.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2/2] Evaluating ART DeepFool...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ART DeepFool:   2%|▏         | 2/100 [01:01<50:22, 30.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "DEEPFOOL RESULTS:\n",
            "----------------------------------------------------------------------\n",
            "Metric                         Custom               ART (SOTA)          \n",
            "----------------------------------------------------------------------\n",
            "Accuracy                       14.00                20.50               \n",
            "Avg Time per Sample (s)        0.1244               30.7677             \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "ART vs Custom:\n",
            "  Accuracy difference: +6.50%\n",
            "  Speed improvement: -24623.4%\n",
            "\n",
            "======================================================================\n",
            "SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Clean Accuracy: 61.79%\n",
            "\n",
            "PGD Attack:\n",
            "  Custom:    2.90% (defender wins if higher)\n",
            "  ART SOTA:  13.40%\n",
            "\n",
            "DeepFool Attack:\n",
            "  Custom:    14.00%\n",
            "  ART SOTA:  20.50%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}